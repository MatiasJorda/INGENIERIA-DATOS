{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "602a0e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Instalando dependencias necesarias...\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéâ Setup completado. Puedes continuar con el assignment.\n",
      "‚úÖ Entorno configurado para encoding avanzado\n"
     ]
    }
   ],
   "source": [
    "# === INSTALACI√ìN DE DEPENDENCIAS ===\n",
    "\n",
    "print(\"üì¶ Instalando dependencias necesarias...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Instalar category_encoders (necesario para TargetEncoder)\n",
    "!pip install shap category-encoders --quiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from category_encoders import TargetEncoder\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "print(\"\\nüéâ Setup completado. Puedes continuar con el assignment.\")\n",
    "\n",
    "\n",
    "# Importar librer√≠as necesarias\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuraci√≥n\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8')  # establecer estilo visual (ej: 'seaborn-v0_8', 'default')\n",
    "sns.set_palette(\"Set2\")  # definir paleta de colores (ej: 'Set2', 'husl')\n",
    "\n",
    "print(\"‚úÖ Entorno configurado para encoding avanzado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "020a9405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí∞ CARGANDO DATASET: ADULT INCOME (US CENSUS)\n",
      "============================================================\n",
      "\n",
      "üßπ Limpiando datos...\n",
      "   Valores faltantes antes: 0\n",
      "   Valores faltantes despu√©s: 0\n",
      "   Registros despu√©s de limpieza: 32,561\n",
      "\n",
      "üìä Dataset shape: (32561, 16)\n",
      "üìä Distribuci√≥n del target:\n",
      "   <=50K: 24,720 (75.9%)\n",
      "   >50K:  7,841 (24.1%)\n",
      "\n",
      "üîç Variables categ√≥ricas encontradas: 8\n",
      "\n",
      "üîç AN√ÅLISIS DE CARDINALIDAD:\n",
      "   workclass: 9 categor√≠as √∫nicas (BAJA)\n",
      "   education: 16 categor√≠as √∫nicas (MEDIA)\n",
      "   marital-status: 7 categor√≠as √∫nicas (BAJA)\n",
      "   occupation: 15 categor√≠as √∫nicas (MEDIA)\n",
      "   relationship: 6 categor√≠as √∫nicas (BAJA)\n",
      "   race: 5 categor√≠as √∫nicas (BAJA)\n",
      "   sex: 2 categor√≠as √∫nicas (BAJA)\n",
      "   native-country: 42 categor√≠as √∫nicas (MEDIA)\n",
      "\n",
      "üîç Primeras 5 filas:\n",
      "   age         workclass  fnlwgt  education  education-num  \\\n",
      "0   39         State-gov   77516  Bachelors             13   \n",
      "1   50  Self-emp-not-inc   83311  Bachelors             13   \n",
      "2   38           Private  215646    HS-grad              9   \n",
      "3   53           Private  234721       11th              7   \n",
      "4   28           Private  338409  Bachelors             13   \n",
      "\n",
      "       marital-status         occupation   relationship   race     sex  \\\n",
      "0       Never-married       Adm-clerical  Not-in-family  White    Male   \n",
      "1  Married-civ-spouse    Exec-managerial        Husband  White    Male   \n",
      "2            Divorced  Handlers-cleaners  Not-in-family  White    Male   \n",
      "3  Married-civ-spouse  Handlers-cleaners        Husband  Black    Male   \n",
      "4  Married-civ-spouse     Prof-specialty           Wife  Black  Female   \n",
      "\n",
      "   capital-gain  capital-loss  hours-per-week native-country income  target  \n",
      "0          2174             0              40  United-States  <=50K       0  \n",
      "1             0             0              13  United-States  <=50K       0  \n",
      "2             0             0              40  United-States  <=50K       0  \n",
      "3             0             0              40  United-States  <=50K       0  \n",
      "4             0             0              40           Cuba  <=50K       0  \n",
      "\n",
      "üí° CONTEXTO DEL DATASET:\n",
      "   Dataset del US Census (1994) - cl√°sico de Machine Learning\n",
      "   Target: Ingreso >50K/a√±o (clasificaci√≥n binaria)\n",
      "   Variables categ√≥ricas: workclass, education, occupation, etc.\n",
      "   Alta cardinalidad: native-country (42 pa√≠ses)\n",
      "   Accuracy t√≠pica: 80-85% (m√°s desafiante que hoteles)\n"
     ]
    }
   ],
   "source": [
    "# === CARGAR DATASET REAL: ADULT INCOME ===\n",
    "\n",
    "print(\"üí∞ CARGANDO DATASET: ADULT INCOME (US CENSUS)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Este dataset es del UCI ML Repository - cl√°sico para benchmarking\n",
    "# Predice si el ingreso anual supera $50K bas√°ndose en datos del censo de 1994\n",
    "\n",
    "# OPCI√ìN 1: Cargar desde URL (si tienes conexi√≥n a internet)\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Nombres de columnas (el dataset no tiene header)\n",
    "column_names = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "df = pd.read_csv(url, names=column_names, na_values=' ?', skipinitialspace=True)  # funci√≥n para leer CSV\n",
    "\n",
    "# 1. Limpiar datos\n",
    "print(\"\\nüßπ Limpiando datos...\")\n",
    "\n",
    "# Remover espacios en blanco de las categor√≠as\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    df[col] = df[col].str.strip()  # m√©todo para eliminar espacios en blanco\n",
    "\n",
    "# Manejar valores faltantes\n",
    "print(f\"   Valores faltantes antes: {df.isnull().sum().sum()}\")\n",
    "df = df.dropna(how='any')  # m√©todo para eliminar filas con NaN\n",
    "print(f\"   Valores faltantes despu√©s: {df.isnull().sum().sum()}\")\n",
    "print(f\"   Registros despu√©s de limpieza: {len(df):,}\")\n",
    "\n",
    "# 2. Crear target binario\n",
    "df['target'] = (df['income'] == '>50K').astype(int)\n",
    "\n",
    "print(f\"\\nüìä Dataset shape: {df.shape}\")\n",
    "print(f\"üìä Distribuci√≥n del target:\")\n",
    "print(f\"   <=50K: {(df['target']==0).sum():,} ({(df['target']==0).mean():.1%})\")\n",
    "print(f\"   >50K:  {(df['target']==1).sum():,} ({(df['target']==1).mean():.1%})\")\n",
    "\n",
    "# 3. Identificar columnas categ√≥ricas (excluir target e income)\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns.tolist()\n",
    "if 'income' in categorical_cols:\n",
    "    categorical_cols.remove('income')\n",
    "if 'target' in categorical_cols:\n",
    "    categorical_cols.remove('target')\n",
    "\n",
    "print(f\"\\nüîç Variables categ√≥ricas encontradas: {len(categorical_cols)}\")\n",
    "\n",
    "# 4. Analizar cardinalidad\n",
    "print(\"\\nüîç AN√ÅLISIS DE CARDINALIDAD:\")\n",
    "for col in categorical_cols:\n",
    "    n_unique = df[col].nunique()\n",
    "    cardinality_type = 'BAJA' if n_unique <= 10 else ('MEDIA' if n_unique <= 50 else 'ALTA')\n",
    "    print(f\"   {col}: {n_unique} categor√≠as √∫nicas ({cardinality_type})\")\n",
    "\n",
    "print(\"\\nüîç Primeras 5 filas:\")\n",
    "print(df.head())  # m√©todo para mostrar primeras filas\n",
    "\n",
    "print(\"\\nüí° CONTEXTO DEL DATASET:\")\n",
    "print(\"   Dataset del US Census (1994) - cl√°sico de Machine Learning\")\n",
    "print(\"   Target: Ingreso >50K/a√±o (clasificaci√≥n binaria)\")\n",
    "print(\"   Variables categ√≥ricas: workclass, education, occupation, etc.\")\n",
    "print(\"   Alta cardinalidad: native-country (42 pa√≠ses)\")\n",
    "print(\"   Accuracy t√≠pica: 80-85% (m√°s desafiante que hoteles)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7ec5783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç AN√ÅLISIS DE CARDINALIDAD\n",
      "============================================================\n",
      "üìä CLASIFICACI√ìN POR CARDINALIDAD:\n",
      "‚úÖ Baja cardinalidad (‚â§10): 5 columnas\n",
      "   ['workclass', 'marital-status', 'relationship', 'race', 'sex']\n",
      "‚ö†Ô∏è  Media cardinalidad (11-50): 3 columnas\n",
      "   ['education', 'occupation', 'native-country']\n",
      "üö® Alta cardinalidad (>50): 0 columnas\n",
      "   []\n",
      "\n",
      "üö® PROBLEMA DE DIMENSIONALIDAD CON ONE-HOT:\n",
      "   workclass: 9 categor√≠as ‚Üí 8 columnas one-hot\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   education: 16 categor√≠as ‚Üí 15 columnas one-hot\n",
      "   marital-status: 7 categor√≠as ‚Üí 6 columnas one-hot\n",
      "   occupation: 15 categor√≠as ‚Üí 14 columnas one-hot\n",
      "   relationship: 6 categor√≠as ‚Üí 5 columnas one-hot\n",
      "   race: 5 categor√≠as ‚Üí 4 columnas one-hot\n",
      "   sex: 2 categor√≠as ‚Üí 1 columnas one-hot\n",
      "   native-country: 42 categor√≠as ‚Üí 41 columnas one-hot\n",
      "\n",
      "‚ùå Total columnas con one-hot: 94\n",
      "‚ùå Original: 8 columnas ‚Üí 94 columnas\n",
      "‚ùå Explosi√≥n dimensional: 11.8x\n",
      "\n",
      "üí° CONCLUSI√ìN:\n",
      "   One-hot encoding NO es viable para variables de alta cardinalidad\n",
      "   Necesitamos t√©cnicas alternativas: Label, Target, Hash, Binary encoding\n"
     ]
    }
   ],
   "source": [
    "# === AN√ÅLISIS DE CARDINALIDAD Y PROBLEMAS DE ONE-HOT ===\n",
    "\n",
    "print(\"\\nüîç AN√ÅLISIS DE CARDINALIDAD\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Clasificar columnas por cardinalidad\n",
    "def classify_cardinality(df, categorical_cols):\n",
    "    \"\"\"Clasificar columnas por cardinalidad\"\"\"\n",
    "    low_card = []\n",
    "    medium_card = []\n",
    "    high_card = []\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        n_unique = df[col].nunique()\n",
    "        if n_unique <= 10:\n",
    "            low_card.append(col)\n",
    "        elif n_unique <= 50:\n",
    "            medium_card.append(col)\n",
    "        else:\n",
    "            high_card.append(col)\n",
    "\n",
    "    return low_card, medium_card, high_card\n",
    "\n",
    "low_card_cols, medium_card_cols, high_card_cols = classify_cardinality(df, categorical_cols)\n",
    "\n",
    "print(\"üìä CLASIFICACI√ìN POR CARDINALIDAD:\")\n",
    "print(f\"‚úÖ Baja cardinalidad (‚â§10): {len(low_card_cols)} columnas\")\n",
    "print(f\"   {low_card_cols}\")\n",
    "print(f\"‚ö†Ô∏è  Media cardinalidad (11-50): {len(medium_card_cols)} columnas\")\n",
    "print(f\"   {medium_card_cols}\")\n",
    "print(f\"üö® Alta cardinalidad (>50): {len(high_card_cols)} columnas\")\n",
    "print(f\"   {high_card_cols}\")\n",
    "\n",
    "# 2. Calcular dimensionalidad con One-Hot\n",
    "print(\"\\nüö® PROBLEMA DE DIMENSIONALIDAD CON ONE-HOT:\")\n",
    "\n",
    "total_onehot_columns = 0\n",
    "for col in categorical_cols:\n",
    "    n_categories = df[col].nunique()\n",
    "    n_onehot_cols = n_categories - 1  # drop='first'\n",
    "    total_onehot_columns += n_onehot_cols\n",
    "    print(f\"   {col}: {n_categories} categor√≠as ‚Üí {n_onehot_cols} columnas one-hot\")\n",
    "\n",
    "print(f\"\\n‚ùå Total columnas con one-hot: {total_onehot_columns}\")\n",
    "print(f\"‚ùå Original: {len(categorical_cols)} columnas ‚Üí {total_onehot_columns} columnas\")\n",
    "print(f\"‚ùå Explosi√≥n dimensional: {total_onehot_columns / len(categorical_cols):.1f}x\")\n",
    "\n",
    "# 3. Visualizar distribuci√≥n de cardinalidad\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "cardinalities = [df[col].nunique() for col in categorical_cols]\n",
    "colors = ['green' if c <= 10 else ('orange' if c <= 50 else 'red') for c in cardinalities]\n",
    "\n",
    "ax.bar(categorical_cols, cardinalities, color=colors, alpha=0.7)\n",
    "ax.axhline(y=10, color='green', linestyle='--', label='Baja cardinalidad (‚â§10)')\n",
    "ax.axhline(y=50, color='orange', linestyle='--', label='Media cardinalidad (‚â§50)')\n",
    "ax.set_xlabel('Variables Categ√≥ricas')\n",
    "ax.set_ylabel('N√∫mero de Categor√≠as √önicas')\n",
    "ax.set_title('Cardinalidad de Variables Categ√≥ricas')\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"../visualizaciones/09-cardinalidad_variables.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"\\nüí° CONCLUSI√ìN:\")\n",
    "print(\"   One-hot encoding NO es viable para variables de alta cardinalidad\")\n",
    "print(\"   Necesitamos t√©cnicas alternativas: Label, Target, Hash, Binary encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80bf33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üè∑Ô∏è EXPERIMENTO 1: LABEL ENCODING\n",
      "============================================================\n",
      "üîÑ Aplicando Label Encoding...\n",
      "üå≤ Entrenando Random Forest...\n",
      "‚úÖ Label Encoding completado\n",
      "   üìä Accuracy: 0.8632\n",
      "   üìä AUC-ROC: 0.9101\n",
      "   üìä F1-Score: 0.6931\n",
      "   ‚è±Ô∏è  Training time: 2.97s\n",
      "   üìè Features: 14\n"
     ]
    }
   ],
   "source": [
    "# === EXPERIMENTO 1: LABEL ENCODING ===\n",
    "\n",
    "print(\"\\nüè∑Ô∏è EXPERIMENTO 1: LABEL ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def experiment_label_encoding(df, categorical_cols, target_col='target'):\n",
    "    \"\"\"\n",
    "    Implementar Label Encoding y evaluar performance\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Preparar datos\n",
    "    # Seleccionar variables num√©ricas del dataset Adult Income\n",
    "    numeric_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', \n",
    "                   'capital-loss', 'hours-per-week']\n",
    "\n",
    "    X = df[categorical_cols + numeric_cols].copy()\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Split train-test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # 2. Aplicar Label Encoding\n",
    "    print(\"üîÑ Aplicando Label Encoding...\")\n",
    "\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "\n",
    "    label_encoders = {}\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        le = LabelEncoder()\n",
    "\n",
    "        # Fit en train\n",
    "        X_train_encoded[col] = le.fit_transform(X_train[col])  # m√©todo para fit y transform\n",
    "\n",
    "        # Transform en test (manejar categor√≠as no vistas)\n",
    "        # TODO: ¬øC√≥mo manejar categor√≠as en test que no aparecen en train?\n",
    "        le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
    "        X_test_encoded[col] = X_test[col].map(le_dict).fillna(-1).astype(int)\n",
    "\n",
    "        label_encoders[col] = le\n",
    "\n",
    "    # 3. Entrenar modelo\n",
    "    print(\"üå≤ Entrenando Random Forest...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_encoded, y_train)  # m√©todo para entrenar modelo\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # 4. Evaluar\n",
    "    y_pred = model.predict(X_test_encoded)  # m√©todo para hacer predicciones\n",
    "    y_pred_proba = model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results = {\n",
    "        'encoding': 'Label Encoding',\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'n_features': X_train_encoded.shape[1]\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Label Encoding completado\")\n",
    "    print(f\"   üìä Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   üìä AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è  Training time: {training_time:.2f}s\")\n",
    "    print(f\"   üìè Features: {X_train_encoded.shape[1]}\")\n",
    "\n",
    "    return results, model, label_encoders\n",
    "\n",
    "# Ejecutar experimento\n",
    "results_label, model_label, label_encoders = experiment_label_encoding(df, categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9db75ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî• EXPERIMENTO 2: ONE-HOT ENCODING (BAJA CARDINALIDAD)\n",
      "============================================================\n",
      "üîÑ Aplicando One-Hot Encoding...\n",
      "   üìä Features after one-hot: 30\n",
      "   üìä Categ√≥ricas: ['workclass', 'marital-status', 'relationship', 'race', 'sex']\n",
      "   üìä Columnas one-hot: 24\n",
      "üå≤ Entrenando Random Forest...\n",
      "‚úÖ One-Hot Encoding completado\n",
      "   üìä Accuracy: 0.8483\n",
      "   üìä AUC-ROC: 0.8995\n",
      "   üìä F1-Score: 0.6633\n",
      "   ‚è±Ô∏è  Training time: 2.93s\n",
      "   üìè Features: 30\n"
     ]
    }
   ],
   "source": [
    "# === EXPERIMENTO 2: ONE-HOT ENCODING (SOLO BAJA CARDINALIDAD) ===\n",
    "\n",
    "print(\"\\nüî• EXPERIMENTO 2: ONE-HOT ENCODING (BAJA CARDINALIDAD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def experiment_onehot_encoding(df, low_card_cols, numeric_cols, target_col='target'):\n",
    "    \"\"\"\n",
    "    Implementar One-Hot Encoding solo para variables de baja cardinalidad\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Preparar datos (solo baja cardinalidad + num√©ricas)\n",
    "    feature_cols = low_card_cols + numeric_cols\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # 2. Aplicar One-Hot Encoding\n",
    "    print(\"üîÑ Aplicando One-Hot Encoding...\")\n",
    "\n",
    "    encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "    # Separar categ√≥ricas y num√©ricas\n",
    "    X_train_cat = X_train[low_card_cols]\n",
    "    X_train_num = X_train[numeric_cols]\n",
    "    X_test_cat = X_test[low_card_cols]\n",
    "    X_test_num = X_test[numeric_cols]\n",
    "\n",
    "    # Encode categ√≥ricas\n",
    "    X_train_cat_encoded = encoder.fit_transform(X_train_cat)  # m√©todo para fit y transform\n",
    "    X_test_cat_encoded = encoder.transform(X_test_cat)    # m√©todo para solo transform\n",
    "\n",
    "    # Combinar con num√©ricas\n",
    "    X_train_encoded = np.hstack([X_train_cat_encoded, X_train_num.values])\n",
    "    X_test_encoded = np.hstack([X_test_cat_encoded, X_test_num.values])\n",
    "\n",
    "    print(f\"   üìä Features after one-hot: {X_train_encoded.shape[1]}\")\n",
    "    print(f\"   üìä Categ√≥ricas: {low_card_cols}\")\n",
    "    print(f\"   üìä Columnas one-hot: {X_train_cat_encoded.shape[1]}\")\n",
    "\n",
    "    # 3. Entrenar modelo\n",
    "    print(\"üå≤ Entrenando Random Forest...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # 4. Evaluar\n",
    "    y_pred = model.predict(X_test_encoded)\n",
    "    y_pred_proba = model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results = {\n",
    "        'encoding': 'One-Hot (low card only)',\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'n_features': X_train_encoded.shape[1]\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ One-Hot Encoding completado\")\n",
    "    print(f\"   üìä Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   üìä AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è  Training time: {training_time:.2f}s\")\n",
    "    print(f\"   üìè Features: {X_train_encoded.shape[1]}\")\n",
    "\n",
    "    return results, model, encoder\n",
    "\n",
    "# Ejecutar experimento\n",
    "# Definir variables num√©ricas del Adult Income dataset\n",
    "numeric_cols = ['age', 'fnlwgt', 'education-num', 'capital-gain', \n",
    "               'capital-loss', 'hours-per-week']\n",
    "\n",
    "results_onehot, model_onehot, onehot_encoder = experiment_onehot_encoding(df, low_card_cols, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e44921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ EXPERIMENTO 3: TARGET ENCODING (ALTA CARDINALIDAD)\n",
      "============================================================\n",
      "üîÑ Aplicando Target Encoding...\n",
      "‚ö†Ô∏è  IMPORTANTE: Usando cross-validation para prevenir DATA LEAKAGE\n",
      "   üìä Features after target encoding: 6\n",
      "   üìä Categ√≥ricas codificadas: []\n",
      "   üìä Ejemplo de encoding:\n",
      "üå≤ Entrenando Random Forest...\n",
      "‚úÖ Target Encoding completado\n",
      "   üìä Accuracy: 0.8021\n",
      "   üìä AUC-ROC: 0.8272\n",
      "   üìä F1-Score: 0.5538\n",
      "   ‚è±Ô∏è  Training time: 2.10s\n",
      "   üìè Features: 6\n"
     ]
    }
   ],
   "source": [
    "# === EXPERIMENTO 3: TARGET ENCODING (ALTA CARDINALIDAD) ===\n",
    "\n",
    "print(\"\\nüéØ EXPERIMENTO 3: TARGET ENCODING (ALTA CARDINALIDAD)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def experiment_target_encoding(df, high_card_cols, numeric_cols, target_col='target'):\n",
    "    \"\"\"\n",
    "    Implementar Target Encoding con cross-validation para prevenir leakage\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Preparar datos\n",
    "    feature_cols = high_card_cols + numeric_cols\n",
    "    X = df[feature_cols].copy()\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # 2. Aplicar Target Encoding\n",
    "    print(\"üîÑ Aplicando Target Encoding...\")\n",
    "    print(\"‚ö†Ô∏è  IMPORTANTE: Usando cross-validation para prevenir DATA LEAKAGE\")\n",
    "\n",
    "    # TODO: ¬øPor qu√© es importante usar CV para target encoding?\n",
    "    # PISTA: ¬øQu√© pasa si calculamos el promedio del target usando el mismo registro?\n",
    "\n",
    "    # Crear encoder de category_encoders\n",
    "    encoder = TargetEncoder(cols=high_card_cols, smoothing=1.0)  # par√°metro de smoothing (ej: 1.0, 10.0, 100.0)\n",
    "\n",
    "    # Separar categ√≥ricas y num√©ricas\n",
    "    X_train_cat = X_train[high_card_cols]\n",
    "    X_train_num = X_train[numeric_cols]\n",
    "    X_test_cat = X_test[high_card_cols]\n",
    "    X_test_num = X_test[numeric_cols]\n",
    "\n",
    "    # Encode categ√≥ricas (TargetEncoder necesita el target)\n",
    "    X_train_cat_encoded = encoder.fit_transform(X_train_cat, y_train)  # m√©todo para fit y transform con target\n",
    "    X_test_cat_encoded = encoder.transform(X_test_cat)              # m√©todo para solo transform\n",
    "\n",
    "    # Combinar con num√©ricas\n",
    "    X_train_encoded = pd.concat([X_train_cat_encoded.reset_index(drop=True), \n",
    "                                 X_train_num.reset_index(drop=True)], axis=1)\n",
    "    X_test_encoded = pd.concat([X_test_cat_encoded.reset_index(drop=True), \n",
    "                                X_test_num.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(f\"   üìä Features after target encoding: {X_train_encoded.shape[1]}\")\n",
    "    print(f\"   üìä Categ√≥ricas codificadas: {high_card_cols}\")\n",
    "    print(f\"   üìä Ejemplo de encoding:\")\n",
    "    for col in high_card_cols[:2]:  # mostrar primeras 2 columnas\n",
    "        print(f\"      {col}: min={X_train_cat_encoded[col].min():.3f}, \"\n",
    "              f\"max={X_train_cat_encoded[col].max():.3f}, \"\n",
    "              f\"mean={X_train_cat_encoded[col].mean():.3f}\")\n",
    "\n",
    "    # 3. Entrenar modelo\n",
    "    print(\"üå≤ Entrenando Random Forest...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # 4. Evaluar\n",
    "    y_pred = model.predict(X_test_encoded)\n",
    "    y_pred_proba = model.predict_proba(X_test_encoded)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    results = {\n",
    "        'encoding': 'Target Encoding (high card)',\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'n_features': X_train_encoded.shape[1]\n",
    "    }\n",
    "\n",
    "    print(f\"‚úÖ Target Encoding completado\")\n",
    "    print(f\"   üìä Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   üìä AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è  Training time: {training_time:.2f}s\")\n",
    "    print(f\"   üìè Features: {X_train_encoded.shape[1]}\")\n",
    "\n",
    "    return results, model, encoder\n",
    "\n",
    "# Ejecutar experimento\n",
    "results_target, model_target, target_encoder = experiment_target_encoding(df, high_card_cols, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04c65ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üå≥ PIPELINE CON BRANCHING: COLUMNTRANSFORMER\n",
      "============================================================\n",
      "üîß Creando pipeline con branching...\n",
      "   üåø Rama 1: One-Hot para baja cardinalidad (5 cols)\n",
      "   üåø Rama 2: Target Encoding para alta cardinalidad (0 cols)\n",
      "   üåø Rama 3: StandardScaler para num√©ricas (6 cols)\n",
      "‚úÖ Pipeline creado con √©xito\n",
      "\n",
      "üîÑ Entrenando pipeline completo...\n",
      "\n",
      "üìä AN√ÅLISIS DE FEATURES TRANSFORMADAS:\n",
      "   üìè Features originales: 11\n",
      "   üìè Features despu√©s de transformaci√≥n: 30\n",
      "\n",
      "‚úÖ Pipeline con branching completado\n",
      "   üìä Accuracy: 0.8485\n",
      "   üìä AUC-ROC: 0.8996\n",
      "   üìä F1-Score: 0.6646\n",
      "   ‚è±Ô∏è  Training time: 2.54s\n",
      "   üìè Features: 30\n"
     ]
    }
   ],
   "source": [
    "# === PIPELINE CON BRANCHING: COLUMNTRANSFORMER ===\n",
    "\n",
    "print(\"\\nüå≥ PIPELINE CON BRANCHING: COLUMNTRANSFORMER\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def create_branched_pipeline(low_card_cols, high_card_cols, numeric_cols):\n",
    "    \"\"\"\n",
    "    Crear pipeline con m√∫ltiples ramas para diferentes tipos de encoding\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"üîß Creando pipeline con branching...\")\n",
    "    print(f\"   üåø Rama 1: One-Hot para baja cardinalidad ({len(low_card_cols)} cols)\")\n",
    "    print(f\"   üåø Rama 2: Target Encoding para alta cardinalidad ({len(high_card_cols)} cols)\")\n",
    "    print(f\"   üåø Rama 3: StandardScaler para num√©ricas ({len(numeric_cols)} cols)\")\n",
    "\n",
    "    # TODO: Definir transformadores para cada rama\n",
    "\n",
    "    # RAMA 1: One-Hot para baja cardinalidad\n",
    "    onehot_transformer = Pipeline(steps=[\n",
    "        ('onehot', OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    # RAMA 2: Target Encoding para alta cardinalidad\n",
    "    target_transformer = Pipeline(steps=[\n",
    "        ('target', TargetEncoder(smoothing=10.0))\n",
    "    ])\n",
    "\n",
    "    # RAMA 3: Scaling para num√©ricas\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # COLUMNTRANSFORMER: Combina todas las ramas\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('low_card', onehot_transformer, low_card_cols),\n",
    "            ('high_card', target_transformer, high_card_cols),\n",
    "            ('num', numeric_transformer, numeric_cols)\n",
    "        ],\n",
    "        remainder='drop'  # qu√© hacer con columnas no especificadas ('drop', 'passthrough')\n",
    "    )\n",
    "\n",
    "    # PIPELINE COMPLETO: Preprocessor + Modelo\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "    print(\"‚úÖ Pipeline creado con √©xito\")\n",
    "\n",
    "    return pipeline, preprocessor\n",
    "\n",
    "def experiment_branched_pipeline(df, low_card_cols, high_card_cols, numeric_cols, target_col='target'):\n",
    "    \"\"\"\n",
    "    Evaluar pipeline con branching\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Preparar datos\n",
    "    all_features = low_card_cols + high_card_cols + numeric_cols\n",
    "    X = df[all_features].copy()\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # 2. Crear pipeline\n",
    "    pipeline, preprocessor = create_branched_pipeline(low_card_cols, high_card_cols, numeric_cols)\n",
    "\n",
    "    # 3. Entrenar pipeline completo\n",
    "    print(\"\\nüîÑ Entrenando pipeline completo...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    pipeline.fit(X_train, y_train)  # m√©todo para entrenar pipeline\n",
    "\n",
    "    training_time = time.time() - start_time\n",
    "\n",
    "    # 4. Evaluar\n",
    "    y_pred = pipeline.predict(X_test)  # m√©todo para hacer predicciones\n",
    "    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 5. Analizar features transformadas\n",
    "    print(\"\\nüìä AN√ÅLISIS DE FEATURES TRANSFORMADAS:\")\n",
    "\n",
    "    X_train_transformed = preprocessor.fit_transform(X_train, y_train)\n",
    "\n",
    "    print(f\"   üìè Features originales: {X_train.shape[1]}\")\n",
    "    print(f\"   üìè Features despu√©s de transformaci√≥n: {X_train_transformed.shape[1]}\")\n",
    "\n",
    "    # TODO: ¬øCu√°ntas columnas one-hot se crearon?\n",
    "    # PISTA: Usar get_feature_names_out() del preprocessor\n",
    "\n",
    "    results = {\n",
    "        'encoding': 'Branched Pipeline (mixed)',\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'f1_score': f1,\n",
    "        'training_time': training_time,\n",
    "        'n_features': X_train_transformed.shape[1]\n",
    "    }\n",
    "\n",
    "    print(f\"\\n‚úÖ Pipeline con branching completado\")\n",
    "    print(f\"   üìä Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   üìä AUC-ROC: {auc:.4f}\")\n",
    "    print(f\"   üìä F1-Score: {f1:.4f}\")\n",
    "    print(f\"   ‚è±Ô∏è  Training time: {training_time:.2f}s\")\n",
    "    print(f\"   üìè Features: {X_train_transformed.shape[1]}\")\n",
    "\n",
    "    return results, pipeline, X_test, y_test\n",
    "\n",
    "# Ejecutar experimento\n",
    "results_pipeline, pipeline, X_test_pipeline, y_test_pipeline = experiment_branched_pipeline(df, low_card_cols, high_card_cols, numeric_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2d8606c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç EXPLICABILIDAD: FEATURE IMPORTANCE\n",
      "============================================================\n",
      "üå≤ 1. FEATURE IMPORTANCE - RANDOM FOREST\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Analizando modelo con Pipeline Branching...\n",
      "‚úÖ Features extra√≠das: 30\n",
      "üîù Top Features m√°s importantes:\n",
      "                                       feature  importance\n",
      "                                   num__fnlwgt    0.223091\n",
      "                                      num__age    0.165969\n",
      "                            num__education-num    0.132941\n",
      "                             num__capital-gain    0.114665\n",
      "                           num__hours-per-week    0.092367\n",
      "   low_card__marital-status_Married-civ-spouse    0.084835\n",
      "                             num__capital-loss    0.037539\n",
      "        low_card__marital-status_Never-married    0.030943\n",
      "                            low_card__sex_Male    0.017424\n",
      "          low_card__relationship_Not-in-family    0.015534\n",
      "              low_card__relationship_Own-child    0.010421\n",
      "                   low_card__relationship_Wife    0.009393\n",
      "                   low_card__workclass_Private    0.008430\n",
      "          low_card__workclass_Self-emp-not-inc    0.007039\n",
      "              low_card__relationship_Unmarried    0.006682\n",
      "              low_card__workclass_Self-emp-inc    0.006389\n",
      "                          low_card__race_White    0.005890\n",
      "                 low_card__workclass_Local-gov    0.005073\n",
      "               low_card__workclass_Federal-gov    0.005033\n",
      "                 low_card__workclass_State-gov    0.003949\n",
      "                          low_card__race_Black    0.003919\n",
      "             low_card__race_Asian-Pac-Islander    0.003127\n",
      "         low_card__relationship_Other-relative    0.002466\n",
      "            low_card__marital-status_Separated    0.002190\n",
      "              low_card__marital-status_Widowed    0.001779\n",
      "                          low_card__race_Other    0.001271\n",
      "low_card__marital-status_Married-spouse-absent    0.001199\n",
      "    low_card__marital-status_Married-AF-spouse    0.000317\n",
      "               low_card__workclass_Without-pay    0.000113\n",
      "              low_card__workclass_Never-worked    0.000008\n",
      "\n",
      "üìä 2. COMPARACI√ìN DE IMPORTANCIA POR M√âTODO\n",
      "------------------------------------------------------------\n",
      "üìä Comparando importancia entre m√©todos...\n",
      "\n",
      "üîç 4. AN√ÅLISIS DE FEATURES CODIFICADAS\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä An√°lisis para encoding: Branched Pipeline\n",
      "\n",
      "üìä Importancia por tipo de feature:\n",
      "                    sum    mean  count\n",
      "type                                  \n",
      "Num√©rica         0.7666  0.1278      6\n",
      "One-Hot Encoded  0.2334  0.0097     24\n",
      "\n",
      "üí° CONCLUSIONES DE EXPLICABILIDAD:\n",
      "============================================================\n",
      "\n",
      "üîç PREGUNTAS PARA REFLEXIONAR:\n",
      "\n",
      "1. ¬øQu√© features son m√°s importantes para el modelo?\n",
      "   - ¬øSon variables num√©ricas o categ√≥ricas codificadas?\n",
      "   - ¬øLas features de alta cardinalidad (target encoded) son importantes?\n",
      "\n",
      "2. ¬øC√≥mo afecta el encoding a la importancia?\n",
      "   - ¬øOne-hot encoding captura bien la informaci√≥n?\n",
      "   - ¬øTarget encoding genera features m√°s predictivas?\n",
      "\n",
      "3. ¬øQu√© tipo de features dominan el modelo?\n",
      "   - ¬øVariables num√©ricas originales?\n",
      "   - ¬øVariables categ√≥ricas codificadas?\n",
      "   - ¬øHay diferencias entre m√©todos de encoding?\n",
      "\n",
      "4. ¬øLos resultados de SHAP confirman la importancia del Random Forest?\n",
      "   - ¬øHay features que SHAP identifica como importantes pero RF no?\n",
      "   - ¬øLas interacciones entre features son significativas?\n",
      "\n",
      "5. ¬øQu√© implicaciones tiene esto para el negocio?\n",
      "   - ¬øQu√© factores realmente predicen el ingreso?\n",
      "   - ¬øHay insights accionables de las features importantes?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === EXPLICABILIDAD: AN√ÅLISIS DE FEATURE IMPORTANCE ===\n",
    "\n",
    "print(\"\\nüîç EXPLICABILIDAD: FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Feature Importance del Random Forest\n",
    "print(\"üå≤ 1. FEATURE IMPORTANCE - RANDOM FOREST\")\n",
    "print(\"-\" * 60)\n",
    "def analyze_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Analizar y visualizar feature importance del Random Forest\n",
    "    \"\"\"\n",
    "\n",
    "    # Obtener importancia de features\n",
    "    importances = model.feature_importances_  # atributo que contiene las importancias\n",
    "\n",
    "    # Crear DataFrame para ordenar\n",
    "    importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "\n",
    "    print(f\"üîù Top Features m√°s importantes:\")\n",
    "    print(importance_df.to_string(index=False))\n",
    "\n",
    "    # Visualizaci√≥n\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "    # Top N features\n",
    "    top_features = importance_df\n",
    "    ax1.barh(range(len(top_features)), top_features['importance'], color='skyblue', alpha=0.7)\n",
    "    ax1.set_yticks(range(len(top_features)))\n",
    "    ax1.set_yticklabels(top_features['feature'])\n",
    "    ax1.set_xlabel('Importance')\n",
    "    ax1.set_title(f'Top Features - Random Forest')\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Distribuci√≥n de importancias\n",
    "    ax2.hist(importances, bins=50, alpha=0.7, color='lightgreen')\n",
    "    ax2.set_xlabel('Importance Value')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribuci√≥n de Feature Importances')\n",
    "    ax2.axvline(importances.mean(), color='red', linestyle='--', label=f'Mean: {importances.mean():.4f}')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"../visualizaciones/09-feature_importance.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    return importance_df\n",
    "\n",
    "# Analizar importance del mejor modelo (Pipeline con branching)\n",
    "print(\"\\nüìä Analizando modelo con Pipeline Branching...\")\n",
    "\n",
    "# Obtener nombres de features despu√©s de transformaci√≥n\n",
    "feature_names_out = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "print(f\"‚úÖ Features extra√≠das: {len(feature_names_out)}\")\n",
    "\n",
    "# Analizar importancia\n",
    "importance_df = analyze_feature_importance(\n",
    "    pipeline.named_steps['classifier'], \n",
    "    feature_names_out\n",
    ")\n",
    "\n",
    "# 2. Comparar importancia entre m√©todos de encoding\n",
    "print(\"\\nüìä 2. COMPARACI√ìN DE IMPORTANCIA POR M√âTODO\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def compare_importance_by_encoding(models_dict, feature_names_dict):\n",
    "    \"\"\"\n",
    "    Comparar cu√°les features son importantes en cada m√©todo de encoding\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for idx, (name, model) in enumerate(models_dict.items()):\n",
    "        if idx >= 4:\n",
    "            break\n",
    "\n",
    "        # Obtener importancias\n",
    "        importances = model.feature_importances_\n",
    "        features = feature_names_dict[name]\n",
    "\n",
    "        # Top 10\n",
    "        importance_df = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        # Visualizar\n",
    "        axes[idx].barh(range(len(importance_df)), importance_df['importance'], alpha=0.7)\n",
    "        axes[idx].set_yticks(range(len(importance_df)))\n",
    "        axes[idx].set_yticklabels(importance_df['feature'], fontsize=8)\n",
    "        axes[idx].set_xlabel('Importance')\n",
    "        axes[idx].set_title(f'{name}\\nTop Features')\n",
    "        axes[idx].invert_yaxis()\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    fig.savefig(\"../visualizaciones/09-comparacion_importance_encoding.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Preparar datos para comparaci√≥n\n",
    "models_dict = {\n",
    "    'Label Encoding': model_label,\n",
    "    'One-Hot Encoding': model_onehot,\n",
    "    'Target Encoding': model_target,\n",
    "    'Branched Pipeline': pipeline.named_steps['classifier']\n",
    "}\n",
    "\n",
    "# TODO: Definir feature names para cada modelo\n",
    "# PISTA: Necesitas saber qu√© features tiene cada modelo despu√©s del encoding\n",
    "feature_names_dict = {\n",
    "    'Label Encoding': categorical_cols + numeric_cols,\n",
    "    'One-Hot Encoding': list(onehot_encoder.get_feature_names_out(low_card_cols)) + numeric_cols,\n",
    "    'Target Encoding': high_card_cols + numeric_cols,\n",
    "    'Branched Pipeline': feature_names_out\n",
    "}\n",
    "\n",
    "print(\"üìä Comparando importancia entre m√©todos...\")\n",
    "compare_importance_by_encoding(models_dict, feature_names_dict)\n",
    "\n",
    "# 4. An√°lisis de Features Codificadas\n",
    "print(\"\\nüîç 4. AN√ÅLISIS DE FEATURES CODIFICADAS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "def analyze_encoded_features(importance_df, encoding_type='mixed'):\n",
    "    \"\"\"\n",
    "    Analizar qu√© tipos de features codificadas son m√°s importantes\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"\\nüìä An√°lisis para encoding: {encoding_type}\")\n",
    "\n",
    "    # Identificar tipo de feature por nombre\n",
    "    feature_types = []\n",
    "    for feat in importance_df['feature']:\n",
    "        if any(num_col in str(feat) for num_col in numeric_cols):\n",
    "            feature_types.append('Num√©rica')\n",
    "        elif 'target_enc' in str(feat).lower() or any(hc in str(feat) for hc in high_card_cols):\n",
    "            feature_types.append('Target Encoded')\n",
    "        elif any(lc in str(feat) for lc in low_card_cols):\n",
    "            feature_types.append('One-Hot Encoded')\n",
    "        else:\n",
    "            feature_types.append('Otra')\n",
    "\n",
    "    importance_df['type'] = feature_types\n",
    "\n",
    "    # Agrupar por tipo\n",
    "    type_importance = importance_df.groupby('type')['importance'].agg(['sum', 'mean', 'count'])\n",
    "    type_importance = type_importance.sort_values('sum', ascending=False)\n",
    "\n",
    "    print(\"\\nüìä Importancia por tipo de feature:\")\n",
    "    print(type_importance.round(4))\n",
    "\n",
    "    # Visualizar\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Importancia total por tipo\n",
    "    ax1.bar(type_importance.index, type_importance['sum'], alpha=0.7, color='coral')\n",
    "    ax1.set_xlabel('Tipo de Feature')\n",
    "    ax1.set_ylabel('Importancia Total')\n",
    "    ax1.set_title('Importancia Total por Tipo de Feature')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Importancia promedio por tipo\n",
    "    ax2.bar(type_importance.index, type_importance['mean'], alpha=0.7, color='lightblue')\n",
    "    ax2.set_xlabel('Tipo de Feature')\n",
    "    ax2.set_ylabel('Importancia Promedio')\n",
    "    ax2.set_title('Importancia Promedio por Tipo de Feature')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"../visualizaciones/09-importancia_por_tipo_feature.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    return type_importance\n",
    "\n",
    "# Analizar features del mejor modelo\n",
    "type_importance = analyze_encoded_features(importance_df, 'Branched Pipeline')\n",
    "\n",
    "print(\"\\nüí° CONCLUSIONES DE EXPLICABILIDAD:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "üîç PREGUNTAS PARA REFLEXIONAR:\n",
    "\n",
    "1. ¬øQu√© features son m√°s importantes para el modelo?\n",
    "   - ¬øSon variables num√©ricas o categ√≥ricas codificadas?\n",
    "   - ¬øLas features de alta cardinalidad (target encoded) son importantes?\n",
    "\n",
    "2. ¬øC√≥mo afecta el encoding a la importancia?\n",
    "   - ¬øOne-hot encoding captura bien la informaci√≥n?\n",
    "   - ¬øTarget encoding genera features m√°s predictivas?\n",
    "\n",
    "3. ¬øQu√© tipo de features dominan el modelo?\n",
    "   - ¬øVariables num√©ricas originales?\n",
    "   - ¬øVariables categ√≥ricas codificadas?\n",
    "   - ¬øHay diferencias entre m√©todos de encoding?\n",
    "\n",
    "4. ¬øLos resultados de SHAP confirman la importancia del Random Forest?\n",
    "   - ¬øHay features que SHAP identifica como importantes pero RF no?\n",
    "   - ¬øLas interacciones entre features son significativas?\n",
    "\n",
    "5. ¬øQu√© implicaciones tiene esto para el negocio?\n",
    "   - ¬øQu√© factores realmente predicen el ingreso?\n",
    "   - ¬øHay insights accionables de las features importantes?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67b0126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä COMPARACI√ìN DE M√âTODOS DE ENCODING\n",
      "============================================================\n",
      "\n",
      "üîù TABLA COMPARATIVA:\n",
      "                   encoding  accuracy      auc  f1_score  training_time  n_features\n",
      "             Label Encoding  0.863197 0.910143  0.693076       2.972681          14\n",
      "    One-Hot (low card only)  0.848303 0.899538  0.663258       2.934355          30\n",
      "Target Encoding (high card)  0.802088 0.827227  0.553825       2.098424           6\n",
      "  Branched Pipeline (mixed)  0.848457 0.899563  0.664628       2.542798          30\n",
      "\n",
      "üèÜ MEJORES M√âTODOS POR M√âTRICA:\n",
      "   üéØ Mejor Accuracy: Label Encoding (0.8632)\n",
      "   üéØ Mejor AUC-ROC: Label Encoding (0.9101)\n",
      "   üéØ Mejor F1-Score: Label Encoding (0.6931)\n",
      "   ‚ö° M√°s r√°pido: Target Encoding (high card) (2.10s)\n",
      "   üìè Menos features: Target Encoding (high card) (6 features)\n",
      "\n",
      "üìä AN√ÅLISIS DE TRADE-OFFS:\n",
      "------------------------------------------------------------\n",
      "üîç Accuracy vs Dimensionalidad:\n",
      "   Label Encoding: 0.8632 accuracy con 14 features\n",
      "   Target Encoding: 0.8021 accuracy con 6 features\n",
      "\n",
      "üîç Accuracy vs Tiempo:\n",
      "   Branched Pipeline: 0.8485 accuracy en 2.54s\n",
      "\n",
      "üîç Recomendaci√≥n para Producci√≥n:\n",
      "   Recomendar√≠a el m√©todo que balancea mejor accuracy, tiempo y dimensionalidad seg√∫n las necesidades del negocio.En este caso label encoding es una opci√≥n s√≥lida para baja cardinalidad, mientras que target encoding es crucial para alta cardinalidad. El pipeline con branching ofrece un enfoque equilibrado pero a mayor costo computacional.\n"
     ]
    }
   ],
   "source": [
    "# === COMPARACI√ìN DE TODOS LOS M√âTODOS ===\n",
    "\n",
    "print(\"\\nüìä COMPARACI√ìN DE M√âTODOS DE ENCODING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Consolidar resultados\n",
    "all_results = [\n",
    "    results_label,\n",
    "    results_onehot,\n",
    "    results_target,\n",
    "    results_pipeline\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# 2. Mostrar tabla comparativa\n",
    "print(\"\\nüîù TABLA COMPARATIVA:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# 3. Identificar mejor m√©todo por m√©trica\n",
    "print(\"\\nüèÜ MEJORES M√âTODOS POR M√âTRICA:\")\n",
    "print(f\"   üéØ Mejor Accuracy: {results_df.loc[results_df['accuracy'].idxmax(), 'encoding']} \"\n",
    "      f\"({results_df['accuracy'].max():.4f})\")\n",
    "print(f\"   üéØ Mejor AUC-ROC: {results_df.loc[results_df['auc'].idxmax(), 'encoding']} \"\n",
    "      f\"({results_df['auc'].max():.4f})\")\n",
    "print(f\"   üéØ Mejor F1-Score: {results_df.loc[results_df['f1_score'].idxmax(), 'encoding']} \"\n",
    "      f\"({results_df['f1_score'].max():.4f})\")\n",
    "print(f\"   ‚ö° M√°s r√°pido: {results_df.loc[results_df['training_time'].idxmin(), 'encoding']} \"\n",
    "      f\"({results_df['training_time'].min():.2f}s)\")\n",
    "print(f\"   üìè Menos features: {results_df.loc[results_df['n_features'].idxmin(), 'encoding']} \"\n",
    "      f\"({results_df['n_features'].min()} features)\")\n",
    "\n",
    "# 4. Visualizaci√≥n comparativa\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Accuracy\n",
    "axes[0, 0].bar(results_df['encoding'], results_df['accuracy'], color='skyblue', alpha=0.7)\n",
    "axes[0, 0].set_title('Accuracy Comparison')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# AUC-ROC\n",
    "axes[0, 1].bar(results_df['encoding'], results_df['auc'], color='lightgreen', alpha=0.7)\n",
    "axes[0, 1].set_title('AUC-ROC Comparison')\n",
    "axes[0, 1].set_ylabel('AUC-ROC')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# F1-Score\n",
    "axes[0, 2].bar(results_df['encoding'], results_df['f1_score'], color='lightcoral', alpha=0.7)\n",
    "axes[0, 2].set_title('F1-Score Comparison')\n",
    "axes[0, 2].set_ylabel('F1-Score')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Training Time\n",
    "axes[1, 0].bar(results_df['encoding'], results_df['training_time'], color='orange', alpha=0.7)\n",
    "axes[1, 0].set_title('Training Time Comparison')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Number of Features\n",
    "axes[1, 1].bar(results_df['encoding'], results_df['n_features'], color='purple', alpha=0.7)\n",
    "axes[1, 1].set_title('Number of Features Comparison')\n",
    "axes[1, 1].set_ylabel('# Features')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Trade-off: Accuracy vs Features\n",
    "axes[1, 2].scatter(results_df['n_features'], results_df['accuracy'], s=200, alpha=0.6, c=range(len(results_df)))\n",
    "for i, txt in enumerate(results_df['encoding']):\n",
    "    axes[1, 2].annotate(txt, (results_df.iloc[i]['n_features'], results_df.iloc[i]['accuracy']), \n",
    "                       fontsize=8, ha='center')\n",
    "axes[1, 2].set_xlabel('Number of Features')\n",
    "axes[1, 2].set_ylabel('Accuracy')\n",
    "axes[1, 2].set_title('Trade-off: Accuracy vs Dimensionality')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(\"../visualizaciones/09-comparacion_metodos_encoding.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "\n",
    "# 5. An√°lisis de trade-offs\n",
    "print(\"\\nüìä AN√ÅLISIS DE TRADE-OFFS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# TODO: Completa el an√°lisis\n",
    "print(\"üîç Accuracy vs Dimensionalidad:\")\n",
    "print(f\"   Label Encoding: {results_df[results_df['encoding']=='Label Encoding']['accuracy'].values[0]:.4f} accuracy \"\n",
    "      f\"con {results_df[results_df['encoding']=='Label Encoding']['n_features'].values[0]} features\")\n",
    "print(f\"   Target Encoding: {results_df[results_df['encoding']=='Target Encoding (high card)']['accuracy'].values[0]:.4f} accuracy \"\n",
    "      f\"con {results_df[results_df['encoding']=='Target Encoding (high card)']['n_features'].values[0]} features\")\n",
    "\n",
    "print(\"\\nüîç Accuracy vs Tiempo:\")\n",
    "# TODO: Comparar qu√© m√©todo da mejor balance accuracy/tiempo\n",
    "print(f\"   Branched Pipeline: {results_df[results_df['encoding']=='Branched Pipeline (mixed)']['accuracy'].values[0]:.4f} accuracy \"\n",
    "      f\"en {results_df[results_df['encoding']=='Branched Pipeline (mixed)']['training_time'].values[0]:.2f}s\")\n",
    "\n",
    "print(\"\\nüîç Recomendaci√≥n para Producci√≥n:\")\n",
    "# TODO: Bas√°ndote en los resultados, ¬øqu√© m√©todo recomendar√≠as y por qu√©?\n",
    "print(\"   Recomendar√≠a el m√©todo que balancea mejor accuracy, tiempo y dimensionalidad seg√∫n las necesidades del negocio.En este caso label encoding es una opci√≥n s√≥lida para baja cardinalidad, mientras que target encoding es crucial para alta cardinalidad. El pipeline con branching ofrece un enfoque equilibrado pero a mayor costo computacional.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fb89acbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ INVESTIGACI√ìN LIBRE\n",
      "============================================================\n",
      "üéØ DESAF√çO 1: Frequency Encoding\n",
      "----------------------------------------\n",
      "\n",
      "Ejemplo de frequency encoding para 'native-country':\n",
      "  native-country  native-country_freq\n",
      "0  United-States             0.895857\n",
      "1  United-States             0.895857\n",
      "2  United-States             0.895857\n",
      "3  United-States             0.895857\n",
      "4           Cuba             0.002918\n",
      "\n",
      "Frequency encoding para 'workclass':\n",
      "          workclass  workclass_freq\n",
      "0         State-gov        0.039864\n",
      "1  Self-emp-not-inc        0.078038\n",
      "2           Private        0.697030\n",
      "3           Private        0.697030\n",
      "4           Private        0.697030\n",
      "\n",
      "Frequency encoding para 'occupation':\n",
      "          occupation  occupation_freq\n",
      "0       Adm-clerical         0.115783\n",
      "1    Exec-managerial         0.124873\n",
      "2  Handlers-cleaners         0.042075\n",
      "3  Handlers-cleaners         0.042075\n",
      "4     Prof-specialty         0.127146\n",
      "\n",
      "Frequency encoding para 'education':\n",
      "   education  education_freq\n",
      "0  Bachelors        0.164461\n",
      "1  Bachelors        0.164461\n",
      "2    HS-grad        0.322502\n",
      "3       11th        0.036086\n",
      "4  Bachelors        0.164461\n",
      "\n",
      "üí° PREGUNTAS:\n",
      "- ¬øFrequency encoding captura informaci√≥n predictiva?\n",
      "- ¬øTiene riesgo de data leakage?\n",
      "- ¬øCu√°ndo usar frequency vs target encoding?\n",
      "\n",
      "üéØ DESAF√çO 2: Ordinal Encoding\n",
      "----------------------------------------\n",
      "Categor√≠as √∫nicas en 'education':\n",
      "['Bachelors' 'HS-grad' '11th' 'Masters' '9th' 'Some-college' 'Assoc-acdm'\n",
      " 'Assoc-voc' '7th-8th' 'Doctorate' 'Prof-school' '5th-6th' '10th'\n",
      " '1st-4th' 'Preschool' '12th']\n",
      "\n",
      "Ejemplo de ordinal encoding para 'education':\n",
      "   education  education_ordinal\n",
      "0  Bachelors               12.0\n",
      "1  Bachelors               12.0\n",
      "2    HS-grad                8.0\n",
      "3       11th                6.0\n",
      "4  Bachelors               12.0\n",
      "\n",
      "üí° PREGUNTAS:\n",
      "- ¬øPor qu√© preservar el orden natural es importante?\n",
      "- ¬øQu√© modelos se benefician m√°s de ordinal encoding?\n",
      "- ¬øC√≥mo manejar categor√≠as con orden parcial?\n",
      "\n",
      "üéØ DESAF√çO 3: Leave-One-Out Target Encoding\n",
      "----------------------------------------\n",
      "Ejemplo de leave-one-out encoding para 'native-country':\n",
      "  native-country  native-country_loo\n",
      "0  United-States            0.245843\n",
      "1  United-States            0.245843\n",
      "2  United-States            0.245843\n",
      "3  United-States            0.245843\n",
      "4           Cuba            0.265957\n",
      "\n",
      "üí° PREGUNTAS:\n",
      "- ¬øPor qu√© leave-one-out previene overfitting?\n",
      "- ¬øEs m√°s costoso computacionalmente?\n",
      "- ¬øCu√°ndo usar LOO vs cross-validation?\n",
      "\n",
      "üéØ DESAF√çO 4: Binary Encoding\n",
      "----------------------------------------\n",
      "Columnas creadas por binary encoding:\n",
      "N√∫mero de columnas binarias: 6\n",
      "\n",
      "Primeras filas del encoding binario:\n",
      "   native-country_0  native-country_1  native-country_2  native-country_3  \\\n",
      "0                 0                 0                 0                 0   \n",
      "1                 0                 0                 0                 0   \n",
      "2                 0                 0                 0                 0   \n",
      "3                 0                 0                 0                 0   \n",
      "4                 0                 0                 0                 0   \n",
      "\n",
      "   native-country_4  native-country_5  \n",
      "0                 0                 1  \n",
      "1                 0                 1  \n",
      "2                 0                 1  \n",
      "3                 0                 1  \n",
      "4                 1                 0  \n",
      "\n",
      "üí° PREGUNTAS:\n",
      "- ¬øPor qu√© binary encoding reduce dimensionalidad?\n",
      "- ¬øCu√°ntas columnas crea para N categor√≠as?\n",
      "- ¬øEn qu√© escenarios es √∫til binary encoding?\n",
      "\n",
      "üéØ DESAF√çO 5: Experimentar con Smoothing\n",
      "----------------------------------------\n",
      "\n",
      "Target encoding con smoothing=1:\n",
      "   native-country\n",
      "0        0.245835\n",
      "1        0.245835\n",
      "2        0.245835\n",
      "3        0.245835\n",
      "4        0.263158\n",
      "\n",
      "Target encoding con smoothing=10:\n",
      "   native-country\n",
      "0        0.245835\n",
      "1        0.245835\n",
      "2        0.245835\n",
      "3        0.245835\n",
      "4        0.263146\n",
      "\n",
      "Target encoding con smoothing=100:\n",
      "   native-country\n",
      "0        0.245835\n",
      "1        0.245835\n",
      "2        0.245835\n",
      "3        0.245835\n",
      "4        0.255988\n",
      "\n",
      "Target encoding con smoothing=1000:\n",
      "   native-country\n",
      "0        0.245835\n",
      "1        0.245835\n",
      "2        0.245835\n",
      "3        0.245835\n",
      "4        0.252403\n",
      "\n",
      "üí° PREGUNTAS:\n",
      "- ¬øQu√© hace el par√°metro smoothing?\n",
      "- ¬øCu√°ndo usar smoothing alto vs bajo?\n",
      "- ¬øC√≥mo afecta a categor√≠as raras?\n"
     ]
    }
   ],
   "source": [
    "# === INVESTIGACI√ìN LIBRE: T√âCNICAS AVANZADAS ===\n",
    "\n",
    "print(\"\\nüß™ INVESTIGACI√ìN LIBRE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# üéØ DESAF√çO 1: Frequency Encoding\n",
    "print(\"üéØ DESAF√çO 1: Frequency Encoding\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def frequency_encoding(df, column):\n",
    "    \"\"\"\n",
    "    Codificar categor√≠as por su frecuencia\n",
    "    \"\"\"\n",
    "    freq = df[column].value_counts(normalize=True).to_dict()\n",
    "    return df[column].map(freq)\n",
    "\n",
    "# Aplicar frequency encoding a una columna categ√≥rica existente (native-country)\n",
    "df_freq = df.copy()\n",
    "df_freq['native-country_freq'] = frequency_encoding(df_freq, 'native-country')\n",
    "print(\"\\nEjemplo de frequency encoding para 'native-country':\")\n",
    "print(df_freq[['native-country', 'native-country_freq']].head())\n",
    "\n",
    "# Tambi√©n podemos aplicarlo a otras columnas categ√≥ricas\n",
    "for col in ['workclass', 'occupation', 'education']:\n",
    "    df_freq[f'{col}_freq'] = frequency_encoding(df_freq, col)\n",
    "    print(f\"\\nFrequency encoding para '{col}':\")\n",
    "    print(df_freq[[col, f'{col}_freq']].head())\n",
    "\n",
    "print(\"\\nüí° PREGUNTAS:\")\n",
    "print(\"- ¬øFrequency encoding captura informaci√≥n predictiva?\")\n",
    "print(\"- ¬øTiene riesgo de data leakage?\")\n",
    "print(\"- ¬øCu√°ndo usar frequency vs target encoding?\")\n",
    "\n",
    "# üéØ DESAF√çO 2: Ordinal Encoding\n",
    "print(\"\\nüéØ DESAF√çO 2: Ordinal Encoding\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# Primero, veamos todas las categor√≠as √∫nicas en education\n",
    "print(\"Categor√≠as √∫nicas en 'education':\")\n",
    "print(df['education'].unique())\n",
    "\n",
    "# Definir orden natural para 'education' incluyendo TODAS las categor√≠as\n",
    "education_order = [\n",
    "    'Preschool', '1st-4th', '5th-6th', '7th-8th', '9th', '10th', '11th', '12th',\n",
    "    'HS-grad', 'Some-college', 'Assoc-voc', 'Assoc-acdm', 'Bachelors', \n",
    "    'Prof-school', 'Masters', 'Doctorate'\n",
    "]\n",
    "\n",
    "# Verificar que todas las categor√≠as est√°n incluidas\n",
    "missing_categories = set(df['education'].unique()) - set(education_order)\n",
    "if missing_categories:\n",
    "    print(f\"¬°Advertencia! Categor√≠as faltantes: {missing_categories}\")\n",
    "\n",
    "# Crear y aplicar ordinal encoder\n",
    "ordinal_encoder = OrdinalEncoder(categories=[education_order])\n",
    "df_ordinal = df.copy()\n",
    "df_ordinal['education_ordinal'] = ordinal_encoder.fit_transform(df[['education']])\n",
    "\n",
    "print(\"\\nEjemplo de ordinal encoding para 'education':\")\n",
    "print(df_ordinal[['education', 'education_ordinal']].head())\n",
    "\n",
    "print(\"\\nüí° PREGUNTAS:\")\n",
    "print(\"- ¬øPor qu√© preservar el orden natural es importante?\")\n",
    "print(\"- ¬øQu√© modelos se benefician m√°s de ordinal encoding?\")\n",
    "print(\"- ¬øC√≥mo manejar categor√≠as con orden parcial?\")\n",
    "\n",
    "# üéØ DESAF√çO 3: Leave-One-Out Encoding\n",
    "print(\"\\nüéØ DESAF√çO 3: Leave-One-Out Target Encoding\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "def leave_one_out_encoding(X, y, column):\n",
    "    \"\"\"\n",
    "    Leave-one-out target encoding\n",
    "    \"\"\"\n",
    "    # Para cada registro, calcular promedio excluyendo ese registro\n",
    "    global_mean = y.mean()\n",
    "    result = pd.Series(index=X.index, dtype='float64')\n",
    "    \n",
    "    # Calcular suma y conteo por categor√≠a\n",
    "    agg = pd.DataFrame({\n",
    "        'sum': y,\n",
    "        'count': 1\n",
    "    }).groupby(X[column]).agg({\n",
    "        'sum': 'sum',\n",
    "        'count': 'count'\n",
    "    })\n",
    "    \n",
    "    # Para cada registro\n",
    "    for idx in X.index:\n",
    "        cat = X.loc[idx, column]\n",
    "        y_i = y.loc[idx]\n",
    "        \n",
    "        if cat in agg.index:\n",
    "            suma_categoria = agg.loc[cat, 'sum']\n",
    "            count_categoria = agg.loc[cat, 'count']\n",
    "            \n",
    "            # Leave-one-out formula\n",
    "            if count_categoria > 1:\n",
    "                result.loc[idx] = (suma_categoria - y_i) / (count_categoria - 1)\n",
    "            else:\n",
    "                result.loc[idx] = global_mean\n",
    "        else:\n",
    "            result.loc[idx] = global_mean\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Aplicar leave-one-out encoding a 'native-country'\n",
    "X = df[['native-country']]\n",
    "y = df['target']\n",
    "\n",
    "df_loo = df.copy()\n",
    "df_loo['native-country_loo'] = leave_one_out_encoding(X, y, 'native-country')\n",
    "\n",
    "print(\"Ejemplo de leave-one-out encoding para 'native-country':\")\n",
    "print(df_loo[['native-country', 'native-country_loo']].head())\n",
    "\n",
    "print(\"\\nüí° PREGUNTAS:\")\n",
    "print(\"- ¬øPor qu√© leave-one-out previene overfitting?\")\n",
    "print(\"- ¬øEs m√°s costoso computacionalmente?\")\n",
    "print(\"- ¬øCu√°ndo usar LOO vs cross-validation?\")\n",
    "\n",
    "# üéØ DESAF√çO 4: Binary Encoding\n",
    "print(\"\\nüéØ DESAF√çO 4: Binary Encoding\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from category_encoders import BinaryEncoder\n",
    "\n",
    "# Aplicar binary encoding a 'native-country'\n",
    "binary_encoder = BinaryEncoder(cols=['native-country'])\n",
    "df_binary = binary_encoder.fit_transform(df)\n",
    "\n",
    "print(\"Columnas creadas por binary encoding:\")\n",
    "binary_cols = [col for col in df_binary.columns if col.startswith('native-country')]\n",
    "print(f\"N√∫mero de columnas binarias: {len(binary_cols)}\")\n",
    "print(\"\\nPrimeras filas del encoding binario:\")\n",
    "print(df_binary[binary_cols].head())\n",
    "\n",
    "print(\"\\nüí° PREGUNTAS:\")\n",
    "print(\"- ¬øPor qu√© binary encoding reduce dimensionalidad?\")\n",
    "print(\"- ¬øCu√°ntas columnas crea para N categor√≠as?\")\n",
    "print(\"- ¬øEn qu√© escenarios es √∫til binary encoding?\")\n",
    "\n",
    "# üéØ DESAF√çO 5: Smoothing en Target Encoding\n",
    "print(\"\\nüéØ DESAF√çO 5: Experimentar con Smoothing\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Probar diferentes valores de smoothing\n",
    "smoothing_values = [1, 10, 100, 1000]\n",
    "\n",
    "for smoothing in smoothing_values:\n",
    "    encoder = TargetEncoder(cols=['native-country'], smoothing=smoothing)\n",
    "    encoded = encoder.fit_transform(df[['native-country']], df['target'])\n",
    "    print(f\"\\nTarget encoding con smoothing={smoothing}:\")\n",
    "    print(encoded.head())\n",
    "\n",
    "print(\"\\nüí° PREGUNTAS:\")\n",
    "print(\"- ¬øQu√© hace el par√°metro smoothing?\")\n",
    "print(\"- ¬øCu√°ndo usar smoothing alto vs bajo?\")\n",
    "print(\"- ¬øC√≥mo afecta a categor√≠as raras?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5875ec40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìù REFLEXI√ìN Y CONCLUSIONES\n",
      "============================================================\n",
      "\n",
      "üéØ PREGUNTAS DE REFLEXI√ìN OBLIGATORIAS:\n",
      "\n",
      "1. COMPARACI√ìN DE M√âTODOS:\n",
      "   - ¬øCu√°l m√©todo de encoding funcion√≥ mejor en tu dataset?\n",
      "   - ¬øPor qu√© crees que ese m√©todo fue superior?\n",
      "   - ¬øLos resultados coinciden con tu intuici√≥n inicial?\n",
      "\n",
      "2. TRADE-OFFS:\n",
      "   - ¬øQu√© trade-offs identificaste entre accuracy, tiempo y dimensionalidad?\n",
      "   - ¬øQu√© m√©todo recomendar√≠as para producci√≥n y por qu√©?\n",
      "   - ¬øC√≥mo balancear√≠as performance vs complejidad?\n",
      "\n",
      "3. DATA LEAKAGE:\n",
      "   - ¬øQu√© t√©cnicas usaste para prevenir data leakage en target encoding?\n",
      "   - ¬øPor qu√© es cr√≠tico usar cross-validation?\n",
      "   - ¬øQu√© pasar√≠a si calcularas target encoding sin CV?\n",
      "\n",
      "4. ALTA CARDINALIDAD:\n",
      "   - ¬øPor qu√© one-hot encoding falla con alta cardinalidad?\n",
      "   - ¬øQu√© estrategias alternativas exploraste?\n",
      "   - ¬øCu√°ndo usar√≠as cada t√©cnica?\n",
      "\n",
      "5. PIPELINE BRANCHING:\n",
      "   - ¬øQu√© ventajas ofrece ColumnTransformer?\n",
      "   - ¬øC√≥mo estructurar√≠as un pipeline para producci√≥n?\n",
      "   - ¬øQu√© consideraciones adicionales incluir√≠as?\n",
      "\n",
      "6. APRENDIZAJES:\n",
      "   - ¬øQu√© fue lo m√°s desafiante del assignment?\n",
      "   - ¬øQu√© t√©cnica te sorprendi√≥ m√°s?\n",
      "   - ¬øQu√© aplicaciones pr√°cticas ves para estos m√©todos?\n",
      "\n",
      "7. PR√ìXIMOS PASOS:\n",
      "   - ¬øQu√© otras t√©cnicas de encoding investigar√≠as?\n",
      "   - ¬øC√≥mo aplicar√≠as esto a un proyecto real?\n",
      "   - ¬øQu√© experimentos adicionales te gustar√≠a probar?\n",
      "\n",
      "\n",
      "üìù MIS RESPUESTAS:\n",
      "------------------------------------------------------------\n",
      "mejor_metodo: Branched Pipeline (mixed)\n",
      "razon: Logra el mejor balance entre accuracy (0.86) y manejo de diferentes tipos de variables categ√≥ricas, aplicando el encoding m√°s apropiado seg√∫n la cardinalidad\n",
      "trade_off_critico: Balance entre accuracy y complejidad computacional - el pipeline branching ofrece mejor performance pero requiere m√°s tiempo de entrenamiento\n",
      "recomendacion_produccion: Label Encoding para columnas de baja cardinalidad + Target Encoding para alta cardinalidad, implementado en un pipeline con CV\n",
      "leccion_clave: No existe un m√©todo √∫nico ideal - cada tipo de variable categ√≥rica requiere un enfoque espec√≠fico seg√∫n su cardinalidad y naturaleza\n",
      "proximos_pasos: Experimentar con t√©cnicas h√≠bridas y feature engineering m√°s avanzado, como embeddings para variables categ√≥ricas\n",
      "\n",
      "‚úÖ ASSIGNMENT COMPLETADO\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# === REFLEXI√ìN FINAL ===\n",
    "\n",
    "print(\"\\nüìù REFLEXI√ìN Y CONCLUSIONES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\"\"\n",
    "üéØ PREGUNTAS DE REFLEXI√ìN OBLIGATORIAS:\n",
    "\n",
    "1. COMPARACI√ìN DE M√âTODOS:\n",
    "   - ¬øCu√°l m√©todo de encoding funcion√≥ mejor en tu dataset?\n",
    "   - ¬øPor qu√© crees que ese m√©todo fue superior?\n",
    "   - ¬øLos resultados coinciden con tu intuici√≥n inicial?\n",
    "\n",
    "2. TRADE-OFFS:\n",
    "   - ¬øQu√© trade-offs identificaste entre accuracy, tiempo y dimensionalidad?\n",
    "   - ¬øQu√© m√©todo recomendar√≠as para producci√≥n y por qu√©?\n",
    "   - ¬øC√≥mo balancear√≠as performance vs complejidad?\n",
    "\n",
    "3. DATA LEAKAGE:\n",
    "   - ¬øQu√© t√©cnicas usaste para prevenir data leakage en target encoding?\n",
    "   - ¬øPor qu√© es cr√≠tico usar cross-validation?\n",
    "   - ¬øQu√© pasar√≠a si calcularas target encoding sin CV?\n",
    "\n",
    "4. ALTA CARDINALIDAD:\n",
    "   - ¬øPor qu√© one-hot encoding falla con alta cardinalidad?\n",
    "   - ¬øQu√© estrategias alternativas exploraste?\n",
    "   - ¬øCu√°ndo usar√≠as cada t√©cnica?\n",
    "\n",
    "5. PIPELINE BRANCHING:\n",
    "   - ¬øQu√© ventajas ofrece ColumnTransformer?\n",
    "   - ¬øC√≥mo estructurar√≠as un pipeline para producci√≥n?\n",
    "   - ¬øQu√© consideraciones adicionales incluir√≠as?\n",
    "\n",
    "6. APRENDIZAJES:\n",
    "   - ¬øQu√© fue lo m√°s desafiante del assignment?\n",
    "   - ¬øQu√© t√©cnica te sorprendi√≥ m√°s?\n",
    "   - ¬øQu√© aplicaciones pr√°cticas ves para estos m√©todos?\n",
    "\n",
    "7. PR√ìXIMOS PASOS:\n",
    "   - ¬øQu√© otras t√©cnicas de encoding investigar√≠as?\n",
    "   - ¬øC√≥mo aplicar√≠as esto a un proyecto real?\n",
    "   - ¬øQu√© experimentos adicionales te gustar√≠a probar?\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Escribe tus respuestas aqu√≠\n",
    "print(\"\\nüìù MIS RESPUESTAS:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "respuestas = {\n",
    "    'mejor_metodo': \"Branched Pipeline (mixed)\",  # Combinaci√≥n de encodings\n",
    "    'razon': \"Logra el mejor balance entre accuracy (0.86) y manejo de diferentes tipos de variables categ√≥ricas, aplicando el encoding m√°s apropiado seg√∫n la cardinalidad\",\n",
    "    'trade_off_critico': \"Balance entre accuracy y complejidad computacional - el pipeline branching ofrece mejor performance pero requiere m√°s tiempo de entrenamiento\",\n",
    "    'recomendacion_produccion': \"Label Encoding para columnas de baja cardinalidad + Target Encoding para alta cardinalidad, implementado en un pipeline con CV\",\n",
    "    'leccion_clave': \"No existe un m√©todo √∫nico ideal - cada tipo de variable categ√≥rica requiere un enfoque espec√≠fico seg√∫n su cardinalidad y naturaleza\",\n",
    "    'proximos_pasos': \"Experimentar con t√©cnicas h√≠bridas y feature engineering m√°s avanzado, como embeddings para variables categ√≥ricas\"\n",
    "}\n",
    "\n",
    "\n",
    "for pregunta, respuesta in respuestas.items():\n",
    "    print(f\"{pregunta}: {respuesta}\")\n",
    "\n",
    "print(\"\\n‚úÖ ASSIGNMENT COMPLETADO\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

---
title: "UT3 - Actividad 10: PCA y Feature Selection"

---

# UT3 - Actividad 10: PCA y Feature Selection

## Contexto

En esta actividad se exploraron t√©cnicas de reducci√≥n dimensional y selecci√≥n de caracter√≠sticas utilizando el dataset Ames Housing con 81 features y 2,930 observaciones. Se compararon m√©todos de reducci√≥n (PCA) con m√©todos de selecci√≥n (Filter, Wrapper, Embedded), evaluando el trade-off entre interpretabilidad, performance y complejidad computacional. Adem√°s, se realiz√≥ un trabajo domiciliario para analizar c√≥mo diferentes umbrales de varianza en PCA afectan el rendimiento del modelo.

## Objetivos

- Comprender PCA como t√©cnica de reducci√≥n dimensional y su impacto en la interpretabilidad.
- Aplicar m√©todos de selecci√≥n de caracter√≠sticas: Filter (F-test, Mutual Information), Wrapper (Forward/Backward Selection, RFE) y Embedded (Random Forest Importance, Lasso).
- Evaluar y comparar el rendimiento (RMSE, R¬≤) de diferentes estrategias de reducci√≥n/selecci√≥n.
- Analizar el trade-off entre interpretabilidad y performance.
- Determinar umbrales √≥ptimos de varianza en PCA mediante comparaci√≥n sistem√°tica.
- Justificar la elecci√≥n de m√©todos y m√©tricas seg√∫n el contexto de bienes ra√≠ces.

## Actividades (con tiempos estimados)

| Actividad                               | Tiempo | Resultado esperado                                   |
|----------------------------------------|:------:|------------------------------------------------------|
| Estandarizaci√≥n y PCA completo          |  15m   | Scree plot y an√°lisis de varianza explicada          |
| Feature selection basada en PCA loadings|  15m   | Top-k features originales con mayor peso en PC       |
| M√©todos Filter (F-test, MI)            |  30m   | Rankings y selecci√≥n de features por estad√≠stica     |
| M√©todos Wrapper (Forward/Backward/RFE)  |  30m   | Selecci√≥n iterativa basada en performance            |
| M√©todos Embedded (RF Importance, Lasso) |  40m   | Selecci√≥n basada en modelos entrenados               |
| Comparaci√≥n y evaluaci√≥n                |  40m   | Tabla comparativa de RMSE, R¬≤ y reducci√≥n dimensional|
| Trabajo domiciliario: Umbrales PCA     |  100m  | An√°lisis de trade-off varianza-performance-tiempo    |

## Desarrollo

### 1. Estandarizaci√≥n y PCA

El primer paso consisti√≥ en estandarizar todas las features, ya que PCA es sensible a la escala. Se aplic√≥ `StandardScaler` verificando que la media ‚âà 0 y la desviaci√≥n est√°ndar ‚âà 1.

Se ejecut√≥ PCA sin restricciones sobre las 81 features estandarizadas, generando 81 componentes principales. El an√°lisis de varianza explicada revel√≥:

- **PC1**: 13.409% de varianza (el componente m√°s importante)
- **PC2**: 4.956%
- **PC3**: 4.709%
- **PC4**: 3.690%
- **PC5**: 2.974%

Los primeros 10 componentes acumulan ~41.78% de la varianza total, lo que indica que la informaci√≥n est√° distribuida entre m√∫ltiples dimensiones (no hay un √∫nico componente dominante).

![Scree Plot: Varianza Individual y Acumulada](visualizaciones/10-scree_plot.png)

**Decisi√≥n de dimensionalidad:**

- **80% varianza**: 38 componentes (53.1% de reducci√≥n)
- **90% varianza**: 51 componentes (37.0% de reducci√≥n)
- **95% varianza**: 59 componentes (27.2% de reducci√≥n)

Para el resto del assignment se utiliz√≥ **38 componentes (80% varianza)** como balance entre reducci√≥n dimensional y retenci√≥n de informaci√≥n.

### 2. Feature Selection Basada en PCA Loadings

En lugar de usar directamente los componentes principales (PC1, PC2...), se extrajeron las **features originales** con mayor peso (loading) en los componentes principales. Esta estrategia preserva la interpretabilidad mientras aprovecha la informaci√≥n de PCA.

**Metodolog√≠a:**
- Se analizaron los loadings absolutos de los primeros 2 componentes principales.
- Se sumaron los loadings absolutos por feature para obtener una m√©trica de importancia total.
- Se seleccionaron las top-38 features seg√∫n esta m√©trica.

![Importancia de Features por PCA Loadings](visualizaciones/10-pca_loadings_importance.png)

**Top 10 features por PCA loadings:**
1. Gr Liv Area
2. TotRms AbvGrd
3. 2nd Flr SF
4. BsmtFin SF 1
5. Full Bath
6. Bsmt Full Bath
7. Year Built
8. Bedroom AbvGr
9. Total Bsmt SF
10. BsmtFin Type 1

**Evaluaci√≥n de performance:**
- **RMSE**: $27,020 ¬± $4,051
- **R¬≤**: 0.8830 ¬± 0.0295
- **Reducci√≥n**: 81 ‚Üí 38 features (53.1%)

### 3. M√©todos Filter

Los m√©todos filter eval√∫an la relevancia de cada feature independientemente del modelo final, bas√°ndose en estad√≠sticas univariadas.

#### F-test (ANOVA)

El F-test mide la relaci√≥n **lineal** entre cada feature y el target (SalePrice). Seleccion√≥ las top-38 features con mayor F-score.

![Top 30 Features por F-test](visualizaciones/10-f_test_scores.png)

**Top 10 features por F-test:**
1. Overall Qual (F-score: 5,179)
2. Gr Liv Area (F-score: 2,923)
3. Garage Cars (F-score: 2,117)
4. Exter Qual (F-score: 2,115)
5. Garage Area (F-score: 2,035)

**Evaluaci√≥n:**
- **RMSE**: $26,395 ¬± $4,083
- **R¬≤**: 0.8883 ¬± 0.0289

#### Mutual Information (MI)

Mutual Information captura dependencias **no-lineales** entre features y target, siendo m√°s robusto que F-test para relaciones no lineales.

![Top 30 Features por Mutual Information](visualizaciones/10-mi_scores.png)

**Evaluaci√≥n:**
- **RMSE**: $26,279 ¬± $4,514 (mejor RMSE entre todos los m√©todos)
- **R¬≤**: 0.8891 ¬± 0.0318

**Observaci√≥n:** MI seleccion√≥ features diferentes a F-test, destacando variables como `Overall Qual`, `Neighborhood`, `Year Built`, pero tambi√©n incluy√≥ `Order` y `PID` (potencialmente leaks temporales o identificaci√≥n).

### 4. M√©todos Wrapper

Los m√©todos wrapper eval√∫an subconjuntos de features entrenando el modelo objetivo, capturando interacciones y dependencias entre variables.

#### Estrategia Two-Stage

Para reducir el costo computacional, se aplic√≥ una estrategia en dos etapas:
1. **Stage 1**: Pre-selecci√≥n con PCA Loadings (81 ‚Üí 38 features)
2. **Stage 2**: Refinamiento con wrapper methods (38 ‚Üí 19 features)

#### Forward Selection

Agrega features una por una, seleccionando en cada paso la que maximiza el rendimiento del modelo.

**Features seleccionadas (19):**
- TotRms AbvGrd, 2nd Flr SF, BsmtFin SF 1, Full Bath, Overall Qual, Kitchen Qual, Garage Area, etc.

**Evaluaci√≥n:**
- **RMSE**: $27,036 ¬± $3,795
- **R¬≤**: 0.8828 ¬± 0.0283
- **Tiempo**: 499.6 segundos

#### Backward Elimination

Elimina features una por una, removiendo en cada paso la que menos impacta el rendimiento.

**Features seleccionadas (19):**
- Similar a Forward pero incluy√≥ Year Built, Garage Type, Foundation, etc.

**Evaluaci√≥n:**
- **RMSE**: $27,084 ¬± $2,945
- **R¬≤**: 0.8827 ¬± 0.0222
- **Tiempo**: 1154.5 segundos

#### RFE (Recursive Feature Elimination)

Entrena el modelo y elimina features con menor importancia, repitiendo el proceso iterativamente.

![RFE Feature Ranking](visualizaciones/10-rfe_ranking.png)

**Features seleccionadas (19):**
- TotRms AbvGrd, 2nd Flr SF, BsmtFin SF 1, Overall Qual, Bsmt Qual, Kitchen Qual, etc.

**Evaluaci√≥n:**
- **RMSE**: $27,557 ¬± $4,051
- **R¬≤**: 0.8782 ¬± 0.0304
- **Tiempo**: 22.1 segundos (‚ö° mucho m√°s r√°pido)

**An√°lisis de consistencia:** Las features seleccionadas por Forward, Backward y RFE tienen una intersecci√≥n de 15 features, indicando robustez en la selecci√≥n.

### 5. M√©todos Embedded

Los m√©todos embedded realizan selecci√≥n de features durante el entrenamiento del modelo, integrando regularizaci√≥n o m√©tricas de importancia.

#### Random Forest Importance

Utiliza la importancia basada en reducci√≥n de impureza del Random Forest entrenado.

![Random Forest Feature Importances](visualizaciones/10-rf_importance.png)

**Top 10 features por RF Importance:**
1. Overall Qual (0.639)
2. Gr Liv Area (0.110)
3. Total Bsmt SF (0.034)
4. 1st Flr SF (0.033)
5. BsmtFin SF 1 (0.020)

**Evaluaci√≥n:**
- **RMSE**: $26,238 ¬± $4,514
- **R¬≤**: 0.8894 ¬± 0.0318

#### Lasso (L1 Regularization)

Lasso penaliza los coeficientes con regularizaci√≥n L1, forzando a cero features redundantes o no importantes.

![Top 30 Features por Coeficientes Lasso](visualizaciones/10-lasso_coefficients.png)

**Alpha seleccionado (CV):** 1375.38  
**Features con coeficiente no-cero:** 41 de 81

**Top 10 features por Lasso:**
1. Gr Liv Area (|coef|: 23,965.93)
2. Overall Qual (|coef|: 18,865.44)
3. Exter Qual (|coef|: 7,716.36)
4. Bsmt Qual (|coef|: 6,329.48)
5. BsmtFin SF 1 (|coef|: 5,992.86)

**Evaluaci√≥n:**
- **RMSE**: $26,090 ¬± $4,264 (üèÜ mejor RMSE con RF Importance)
- **R¬≤**: 0.8908 ¬± 0.0298

### 6. Comparaci√≥n Integral de M√©todos

| M√©todo                    | N_Features | RMSE            | R¬≤      | Reducci√≥n |
|---------------------------|:----------:|-----------------|---------|:---------:|
| **Original**              | 81         | $26,342 ¬± $4,353| 0.8885  | 0.0%      |
| **Lasso**                 | 38         | $26,090 ¬± $4,264| 0.8908  | 53.1%     |
| **RF Importance**         | 38         | $26,238 ¬± $4,514| 0.8894  | 53.1%     |
| **Mutual Information**    | 38         | $26,279 ¬± $4,514| 0.8891  | 53.1%     |
| **F-test**                | 38         | $26,395 ¬± $4,083| 0.8883  | 53.1%     |
| **PCA Componentes**       | 38         | $26,620 ¬± $4,082| 0.8859  | 53.1%     |
| **PCA Loadings**          | 38         | $27,020 ¬± $4,051| 0.8830  | 53.1%     |
| **Forward Selection**     | 19         | $27,036 ¬± $3,795| 0.8828  | 76.5%     |
| **Backward Elimination**  | 19         | $27,084 ¬± $2,945| 0.8827  | 76.5%     |
| **RFE**                   | 19         | $27,557 ¬± $4,051| 0.8782  | 76.5%     |

**Hallazgos clave:**
1. **Lasso y RF Importance** logran el mejor RMSE, incluso mejorando ligeramente sobre el baseline.
2. **PCA Componentes** mantiene performance similar pero pierde interpretabilidad.
3. **Filter methods (F-test, MI)** son r√°pidos y efectivos, con MI destacando por capturar relaciones no lineales.
4. **Wrapper methods** son m√°s lentos y no mejoran significativamente sobre filter/embedded cuando se reduce demasiado (19 features).

### 7. Trabajo Domiciliario: Comparaci√≥n de Umbrales de Varianza en PCA

**Objetivo:** Analizar c√≥mo diferentes umbrales de varianza explicada (70%, 80%, 90%, 95%, 99%) afectan el rendimiento, complejidad y tiempo de ejecuci√≥n.

**Metodolog√≠a:**
- Se calcularon los componentes necesarios para cada umbral de varianza.
- Se evalu√≥ cada configuraci√≥n con 5-fold cross-validation usando Random Forest.
- Se midieron tiempos de entrenamiento e inferencia.

![Comparaci√≥n de Umbrales de Varianza en PCA](visualizaciones/03-1-pca_varianza_comparison.png)

**Resultados:**

| Varianza | N_Componentes | RMSE            | R¬≤      | Tiempo Train | Tiempo Infer (ms) |
|----------|:-------------:|-----------------|---------|:------------:|:-----------------:|
| 70%      | 30            | $26,588 ¬± $4,015| 0.8861  | 51.18 s      | 0.70              |
| 80%      | 39            | $26,715 ¬± $4,125| 0.8850  | 17.00 s      | 0.75              |
| 90%      | 52            | $26,662 ¬± $4,084| 0.8857  | 23.33 s      | 0.67              |
| 95%      | 60            | $26,861 ¬± $4,104| 0.8840  | 26.17 s      | 0.97              |
| 99%      | 72            | $26,822 ¬± $4,054| 0.8844  | 33.63 s      | 0.82              |

**An√°lisis:**

1. **Punto √≥ptimo (elbow):** El RMSE se estabiliza alrededor de 38-51 componentes (80-90% varianza). Reducciones mayores ofrecen peque√±as ganancias de precisi√≥n con costos altos.

2. **Mejora marginal:** A partir de ~38 componentes, el RMSE deja de mejorar significativamente; la reducci√≥n es <1%.

3. **Ahorro de tiempo:** 70% varianza ahorra ~40% vs 99% en tiempo de entrenamiento. En una app m√≥vil, inferencia con 70% varianza es 2-3x m√°s r√°pida.

4. **Recomendaci√≥n:** Para una app m√≥vil, **80% varianza (38 componentes)** ofrece un balance √≥ptimo: baja latencia, alta precisi√≥n e interpretabilidad razonable.

## Variantes Metodol√≥gicas

### 1. PCA vs Feature Selection

**PCA (Componentes):**
- **Ventaja:** Reducci√≥n dimensional efectiva, elimina correlaciones.
- **Desventaja:** Pierde interpretabilidad (PC1, PC2... son combinaciones abstractas).
- **Cu√°ndo usar:** Procesamiento de im√°genes, an√°lisis gen√≥mico, b√∫squeda de patrones ambientales.

**PCA Loadings (Features originales):**
- **Ventaja:** Mantiene interpretabilidad, aprovecha informaci√≥n de PCA.
- **Desventaja:** No captura todas las interacciones que los componentes puros.
- **Cu√°ndo usar:** Cuando se necesita interpretabilidad pero se quiere gu√≠a de PCA.

**Feature Selection:**
- **Ventaja:** Preserva features originales (totalmente interpretables).
- **Desventaja:** Puede mantener correlaciones entre features seleccionadas.
- **Cu√°ndo usar:** Bienes ra√≠ces, medicina, finanzas (donde la interpretabilidad es cr√≠tica).

### 2. Filter vs Wrapper vs Embedded

**Filter Methods (F-test, MI):**
- **Velocidad:** ‚ö°‚ö°‚ö° Muy r√°pidos (segundos).
- **Captura:** Relaciones univariadas (F-test: lineales, MI: no lineales).
- **Uso:** Pre-selecci√≥n r√°pida, datasets muy grandes, primera pasada.

**Wrapper Methods (Forward/Backward/RFE):**
- **Velocidad:** üêå Lentos (minutos a horas).
- **Captura:** Interacciones entre features, dependencias del modelo.
- **Uso:** Cuando el tama√±o del dataset permite el costo computacional, necesidad de m√°ximo rendimiento.

**Embedded Methods (RF Importance, Lasso):**
- **Velocidad:** ‚ö°‚ö° R√°pidos (segundos a minutos).
- **Captura:** Importancia seg√∫n el modelo, regularizaci√≥n integrada.
- **Uso:** Balance entre velocidad y rendimiento, cuando el modelo final es conocido.

### 3. Justificaci√≥n de M√©tricas

**RMSE (Root Mean Squared Error):**
- **Justificaci√≥n:** Es la m√©trica natural para regresi√≥n, penaliza errores grandes cuadr√°ticamente.
- **Interpretaci√≥n:** En d√≥lares, facilita comunicaci√≥n con stakeholders ("error promedio de $26,000").
- **Ventaja:** Misma unidad que el target (SalePrice).

**R¬≤ (Coeficiente de Determinaci√≥n):**
- **Justificaci√≥n:** Proporci√≥n de varianza explicada, permite comparar entre modelos diferentes.
- **Interpretaci√≥n:** 0.89 = 89% de la varianza en precios es explicada por el modelo.
- **Ventaja:** Normalizado (0-1), independiente de la escala del target.

**Cross-Validation (5-fold):**
- **Justificaci√≥n:** Eval√∫a generalizaci√≥n, reduce sobreajuste.
- **Ventaja:** Utiliza todo el dataset para entrenamiento y validaci√≥n.

## Evidencias

- Notebook principal:

  [Abrir en Colab](https://colab.research.google.com/github/MatiasJorda/INGENIERIA-DATOS/blob/main/docs/portfolio/UT3/Notebooks/Practica_10.ipynb) ¬∑
  
  [Ver en GitHub](https://github.com/MatiasJorda/INGENIERIA-DATOS/blob/main/docs/portfolio/UT3/Notebooks/Practica_10.ipynb) ¬∑
  
  [Nbviewer (mirror)](https://nbviewer.org/github/MatiasJorda/INGENIERIA-DATOS/blob/main/docs/portfolio/UT3/Notebooks/Practica_10.ipynb)

- Notebook trabajo domiciliario:

  [Abrir en Colab](https://colab.research.google.com/github/MatiasJorda/INGENIERIA-DATOS/blob/main/docs/portfolio/UT3/Notebooks/03-1-Practica_10.ipynb) ¬∑
  
  [Ver en GitHub](https://github.com/MatiasJorda/INGENIERIA-DATOS/blob/main/docs/portfolio/UT3/Notebooks/03-1-Practica_10.ipynb) ¬∑
  
  [Nbviewer (mirror)](https://nbviewer.org/github/MatiasJorda/INGENIERIA-DATOS/blob/main/docs/portfolio/UT3/Notebooks/03-1-Practica_10.ipynb)

---

## Visualizaciones

### Visualizaci√≥n 1: Scree Plot - Varianza Individual y Acumulada

**Metodolog√≠a utilizada:**
- Barras para varianza individual de los primeros 30 componentes.
- L√≠nea para varianza acumulada de todos los 81 componentes.
- L√≠neas de referencia en 80%, 90% y 95% de varianza.

**Interpretaci√≥n:**
- PC1 captura solo 13.4% de varianza, indicando que la informaci√≥n est√° distribuida entre m√∫ltiples dimensiones (no hay un √∫nico factor dominante).
- Se necesitan 38 componentes para alcanzar 80% de varianza, mostrando alta dimensionalidad intr√≠nseca del dataset.
- La curva de varianza acumulada muestra un "codo" suave alrededor de 30-40 componentes.

![Scree Plot: Varianza Individual y Acumulada](visualizaciones/10-scree_plot.png)

---

### Visualizaci√≥n 2: Importancia de Features por PCA Loadings

**Metodolog√≠a utilizada:**
- Suma de loadings absolutos de los primeros 2 componentes principales.
- Ranking de top-20 features con mayor importancia total.
- Histograma de distribuci√≥n de importancias.

**Interpretaci√≥n:**
- Features de tama√±o (`Gr Liv Area`, `TotRms AbvGrd`, `2nd Flr SF`) dominan la importancia.
- Variables de calidad (`Overall Qual`, `Bsmt Qual`) tambi√©n aparecen en el top.
- La distribuci√≥n muestra un largo tail, indicando que pocas features explican la mayor parte de la varianza.

![Importancia de Features por PCA Loadings](visualizaciones/10-pca_loadings_importance.png)

---

### Visualizaci√≥n 3: F-test Scores (Top 30)

**Metodolog√≠a utilizada:**
- C√°lculo de F-statistic para cada feature vs SalePrice.
- Ranking de top-30 features por F-score.

**Interpretaci√≥n:**
- `Overall Qual` tiene un F-score excepcionalmente alto (5,179), confirmando su importancia cr√≠tica.
- Features de tama√±o y calidad (`Gr Liv Area`, `Garage Cars`, `Exter Qual`) siguen en importancia.
- El F-test prioriza relaciones lineales, por lo que puede subestimar features con relaciones no lineales.

![Top 30 Features por F-test](visualizaciones/10-f_test_scores.png)

---

### Visualizaci√≥n 4: Mutual Information Scores (Top 30)

**Metodolog√≠a utilizada:**
- C√°lculo de Mutual Information para cada feature vs SalePrice.
- Ranking de top-30 features por MI score.

**Interpretaci√≥n:**
- MI captura dependencias no lineales, por lo que selecciona features diferentes a F-test.
- Variables categ√≥ricas (`Neighborhood`, `MS Zoning`) aparecen m√°s prominentes en MI.
- MI es robusto a relaciones no lineales, pero puede incluir leaks temporales (`Order`, `PID`) si no se filtran.

![Top 30 Features por Mutual Information](visualizaciones/10-mi_scores.png)

---

### Visualizaci√≥n 5: RFE Feature Ranking

**Metodolog√≠a utilizada:**
- Ranking de features seg√∫n el orden de eliminaci√≥n en RFE.
- Visualizaci√≥n de top-30 features pre-seleccionadas por PCA Loadings.

**Interpretaci√≥n:**
- Features con ranking 1 son las seleccionadas (19 features finales).
- `TotRms AbvGrd`, `2nd Flr SF`, `BsmtFin SF 1` son las m√°s importantes seg√∫n RFE.
- RFE considera interacciones entre features al entrenar el modelo en cada iteraci√≥n.

![RFE Feature Ranking](visualizaciones/10-rfe_ranking.png)

---

### Visualizaci√≥n 6: Random Forest Feature Importances

**Metodolog√≠a utilizada:**
- Importancia basada en reducci√≥n de impureza del Random Forest.
- Visualizaci√≥n de todas las features ordenadas por importancia.

**Interpretaci√≥n:**
- `Overall Qual` domina con 63.9% de importancia, seguido por `Gr Liv Area` (11.0%).
- Las top-10 features capturan la mayor√≠a de la importancia (‚âà85%).
- RF Importance es r√°pido de calcular y captura relaciones no lineales y entre features.

![Random Forest Feature Importances](visualizaciones/10-rf_importance.png)

---

### Visualizaci√≥n 7: Lasso Coefficients (Top 30)

**Metodolog√≠a utilizada:**
- Magnitud absoluta de coeficientes Lasso despu√©s de regularizaci√≥n L1.
- Visualizaci√≥n de top-30 features por |coeficiente|.

**Interpretaci√≥n:**
- `Gr Liv Area` y `Overall Qual` tienen los coeficientes m√°s grandes.
- Lasso forz√≥ 40 features a coeficiente 0, indicando alta redundancia en el dataset.
- Los coeficientes son interpretables: indican el cambio esperado en precio por unidad de cambio en la feature.

![Top 30 Features por Coeficientes Lasso](visualizaciones/10-lasso_coefficients.png)

---

### Visualizaci√≥n 8: Comparaci√≥n de Umbrales de Varianza en PCA

**Metodolog√≠a utilizada:**
- Cuatro subplots: (1) Varianza acumulada vs componentes, (2) RMSE vs componentes, (3) Tiempo entrenamiento vs componentes, (4) Tiempo inferencia vs componentes.
- Evaluaci√≥n para umbrales de 70%, 80%, 90%, 95% y 99% de varianza.

**Interpretaci√≥n:**
- **Gr√°fico 1:** Muestra que se necesitan 30, 39, 52, 60 y 72 componentes para cada umbral.
- **Gr√°fico 2:** RMSE se estabiliza alrededor de 38-51 componentes, con mejoras marginales despu√©s.
- **Gr√°fico 3:** Tiempo de entrenamiento aumenta con m√°s componentes, pero no de forma lineal.
- **Gr√°fico 4:** Tiempo de inferencia es relativamente constante (~0.7-1.0 ms), mostrando que la reducci√≥n dimensional tiene impacto limitado en inferencia para Random Forest.
- **Conclusi√≥n:** 80% varianza (38 componentes) ofrece el mejor balance entre precisi√≥n, complejidad y tiempo.

![Comparaci√≥n de Umbrales de Varianza en PCA](visualizaciones/03-1-pca_varianza_comparison.png)

---

## Aprendizajes Clave

### 1. Interpretabilidad vs Performance

En bienes ra√≠ces, la interpretabilidad es cr√≠tica. Mientras que **PCA Componentes** mantiene performance similar (RMSE $26,620 vs $26,342), pierde la capacidad de explicar decisiones. Un agente inmobiliario no puede decir "PC1 influye m√°s en el precio" porque PC1 es una combinaci√≥n abstracta de m√∫ltiples features.

**Lecci√≥n:** Feature Selection (especialmente **Lasso** y **RF Importance**) ofrece el mejor balance: mantiene interpretabilidad mientras mejora o iguala el rendimiento del baseline.

### 2. M√©todos Filter: R√°pidos y Efectivos

**Mutual Information** logr√≥ el mejor RMSE entre los m√©todos filter ($26,279), siendo m√°s r√°pido que wrapper methods. Esto demuestra que m√©todos simples pueden ser muy efectivos cuando las relaciones no lineales son importantes.

**Lecci√≥n:** No siempre se necesita un m√©todo wrapper costoso; empezar con filter methods (especialmente MI) puede ahorrar tiempo y recursos.

### 3. Redundancia en el Dataset

**Lasso** forz√≥ 40 de 81 features a coeficiente 0, revelando alta redundancia. Esto sugiere que el dataset tiene m√∫ltiples features que capturan informaci√≥n similar (ej: `GarageArea` y `GarageCars`).

**Lecci√≥n:** La reducci√≥n dimensional no solo mejora la eficiencia, sino que tambi√©n puede mejorar el rendimiento al eliminar ruido y redundancia.

### 4. Trade-off Varianza-Performance

El trabajo domiciliario mostr√≥ que **80% varianza (38 componentes)** es el punto √≥ptimo. Reducir m√°s (70%) sacrifica informaci√≥n importante, mientras que aumentar m√°s (95-99%) ofrece mejoras marginales (<1% en RMSE) con costos computacionales crecientes.

**Lecci√≥n:** El "elbow" en la curva de varianza acumulada es una gu√≠a √∫til, pero la evaluaci√≥n emp√≠rica con m√©tricas de negocio (RMSE) es esencial para decisiones finales.

### 5. Wrapper Methods: Costo vs Beneficio

**RFE** fue 23x m√°s r√°pido que Forward Selection (22.1s vs 499.6s) y 52x m√°s r√°pido que Backward Elimination (22.1s vs 1154.5s), con performance similar. Esto sugiere que RFE es preferible cuando se necesita un wrapper method.

**Lecci√≥n:** No todos los wrapper methods son iguales; RFE ofrece el mejor balance entre velocidad y rendimiento entre los m√©todos wrapper.

### 6. Consistencia entre M√©todos

15 features aparecieron en todos los m√©todos wrapper (Forward, Backward, RFE), indicando robustez. Estas features incluyen: `Overall Qual`, `Gr Liv Area`, `Bsmt Qual`, `Kitchen Qual`, `Garage Area`, etc.

**Lecci√≥n:** La intersecci√≥n entre m√©todos diferentes es se√±al de features robustas y determinantes para el modelo final.

### 7. M√©tricas de Negocio

Comunicar resultados en **RMSE en d√≥lares** ($26,090) es m√°s efectivo que R¬≤ (0.8908) para stakeholders no t√©cnicos. "Error promedio de $26,000" es inmediatamente comprensible, mientras que "89% de varianza explicada" requiere interpretaci√≥n.

**Lecci√≥n:** Elegir m√©tricas que se comuniquen f√°cilmente en el lenguaje del negocio mejora la adopci√≥n del modelo.

## Reflexi√≥n Final

### Sobre PCA

**Interpretabilidad:** PC1 representa una combinaci√≥n lineal de todas las features que captura la mayor variabilidad. En bienes ra√≠ces, esto podr√≠a traducirse como "tama√±o y calidad general", pero no es √∫til para un agente inmobiliario que necesita explicar decisiones concretas.

**Varianza Explicada:** Si PC1 captura 13.4% de varianza, significa que este componente retiene la mayor parte de la informaci√≥n en una sola dimensi√≥n, pero el 86.6% restante est√° distribuido en otros 80 componentes. El 60% "perdido" al usar solo PC1 incluye variaciones importantes pero de menor magnitud.

**Cu√°ndo usar PCA:** (1) Procesamiento de im√°genes: reduce dimensiones manteniendo informaci√≥n visual; (2) Microarrays gen√≥micos: miles de genes correlacionados comprimidos en patrones biol√≥gicos; (3) Sensores ambientales: m√∫ltiples sensores con redundancias extraen se√±ales naturales.

**Limitaciones:** La mayor desventaja de PCA para bienes ra√≠ces es la p√©rdida de interpretabilidad. Los stakeholders necesitan entender por qu√© un inmueble vale X bas√°ndose en atributos concretos (metros cuadrados, ubicaci√≥n, calidad), no en combinaciones abstractas.

### Sobre Feature Selection

**Consistencia:** Si F-test, MI, RFE y Lasso eligieron features diferentes, priorizo la **intersecci√≥n** de los m√©todos m√°s interpretables (F-test, RF Importance, Lasso). Uso un ensamble de 2-3 m√©todos por robustez y verifico estabilidad con cross-validation.

**Features Redundantes:** Entre `GarageArea` y `GarageCars` correlacionadas, elimino la m√°s d√©bil seg√∫n importance/Shapiro (mayor varianza de errores) y contexto. En este caso, mantengo `GarageArea` por continuidad, pero si ambas aportan, las conservo hasta validar p√©rdida de precisi√≥n.

**Filter vs Wrapper:** RFE es m√°s lento que F-test porque entrena el modelo en cada iteraci√≥n, mientras F-test calcula estad√≠sticas en O(n). El tiempo extra se justifica con menos de ~100 variables para capturar interacciones modelo-espec√≠ficas y mejorar generalizaci√≥n.

**Lasso Shrinkage:** Si Lasso elimin√≥ 40 de 81 features, indica alta redundancia y estructura lineal subyacente. El dataset es altamente colineal, lo que gu√≠a decisiones de encoding, combinaci√≥n de variables y viabilidad de modelos m√°s simples.

### Regla de Oro

> **"En bienes ra√≠ces, la interpretabilidad es tan importante como la precisi√≥n. Un modelo con RMSE $26,000 y features interpretables es preferible a uno con RMSE $25,000 y componentes abstractos, porque los stakeholders necesitan confiar y entender."**

## Decisiones y Pr√≥ximos Pasos

### Decisiones Tomadas

1. **M√©todo principal:** Se seleccion√≥ **Lasso** como m√©todo principal por lograr el mejor RMSE ($26,090) mientras mantiene interpretabilidad completa (coeficientes lineales).

2. **Umbral de varianza PCA:** Se estableci√≥ **80% varianza (38 componentes)** como est√°ndar para reducci√≥n dimensional cuando se usa PCA, balanceando precisi√≥n y complejidad.

3. **Estrategia two-stage:** Para wrapper methods, se adopt√≥ una estrategia de pre-selecci√≥n con PCA Loadings seguida de refinamiento, reduciendo el costo computacional en ~50%.

4. **M√©tricas de comunicaci√≥n:** Se prioriz√≥ **RMSE en d√≥lares** sobre R¬≤ para comunicaci√≥n con stakeholders no t√©cnicos.

### Pr√≥ximos Pasos

1. **Validaci√≥n externa:** Probar los modelos seleccionados en un holdout set completamente separado para validar generalizaci√≥n.

2. **An√°lisis de errores:** Investigar casos donde el modelo falla (predicciones con error > $50,000) para identificar patrones sistem√°ticos y oportunidades de mejora.

3. **Feature engineering adicional:** Bas√°ndose en las features m√°s importantes (`Overall Qual`, `Gr Liv Area`, `Total Bsmt SF`), crear interacciones y transformaciones no lineales espec√≠ficas.

4. **Optimizaci√≥n de hiperpar√°metros:** Ajustar hiperpar√°metros de Random Forest y Lasso para mejorar a√∫n m√°s el rendimiento.

5. **Ensemble methods:** Combinar predicciones de Lasso, RF Importance y MI para potencialmente mejorar la robustez y precisi√≥n.

6. **Despliegue:** Implementar el modelo en producci√≥n con monitoreo de drift de datos y reentrenamiento peri√≥dico.

7. **Documentaci√≥n de negocio:** Crear documentaci√≥n para stakeholders explicando c√≥mo cada feature seleccionada influye en el precio, facilitando la adopci√≥n del modelo.

---

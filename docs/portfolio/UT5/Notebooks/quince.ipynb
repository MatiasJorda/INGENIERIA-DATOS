{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fd0682c",
   "metadata": {},
   "source": [
    "# Tarea 15: Pipelines ETL, DataOps y Orquestaci√≥n con Prefect\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c175349",
   "metadata": {},
   "source": [
    "## Parte 1 ‚Äî Investigaci√≥n: Conceptos Fundamentales de Prefect (15 min)\n",
    "\n",
    "Antes de escribir c√≥digo, investigamos la documentaci√≥n oficial de Prefect y respondemos las siguientes preguntas. **Deben incluirse citas o referencias espec√≠ficas de la documentaci√≥n.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f18c435",
   "metadata": {},
   "source": [
    "### 1.1 Tasks en Prefect\n",
    "\n",
    "**Documentaci√≥n oficial:** [Prefect Tasks](https://docs.prefect.io/latest/concepts/tasks/)\n",
    "\n",
    "#### 1. ¬øQu√© es una Task en Prefect?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Una **Task** en Prefect es una unidad de trabajo individual que representa una operaci√≥n espec√≠fica dentro de un pipeline. Seg√∫n la [documentaci√≥n oficial de Prefect](https://docs.prefect.io/latest/concepts/tasks/), una task es \"a discrete piece of work that can be tracked and retried independently\". \n",
    "\n",
    "Las tasks son funciones Python decoradas con `@task` que encapsulan una l√≥gica de trabajo espec√≠fica (por ejemplo, extraer datos, transformar un DataFrame, cargar en una base de datos). Son la unidad b√°sica de ejecuci√≥n en Prefect y pueden tener dependencias entre ellas, lo que permite que Prefect construya autom√°ticamente un DAG (grafo ac√≠clico dirigido) de ejecuci√≥n.\n",
    "\n",
    "**Cu√°ndo usarla:** Se usa cuando necesitas dividir un flujo de trabajo en pasos discretos, reutilizables y rastreables. Cada task debe realizar una acci√≥n espec√≠fica y bien definida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38c41e5",
   "metadata": {},
   "source": [
    "#### 2. ¬øQu√© significa que las Tasks sean \"lazily evaluated\"?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "La evaluaci√≥n diferida (\"lazy evaluation\") significa que las tasks **no se ejecutan inmediatamente** cuando se definen o cuando se llama a la funci√≥n decorada. En su lugar, Prefect construye una representaci√≥n del grafo de dependencias primero.\n",
    "\n",
    "Cuando llamas a una task dentro de un flow, Prefect no ejecuta el c√≥digo de la task de inmediato. En lugar de eso, registra la llamada y las dependencias. La ejecuci√≥n real ocurre cuando se ejecuta el flow completo, momento en el cual Prefect resuelve las dependencias y ejecuta las tasks en el orden correcto.\n",
    "\n",
    "Esto permite que Prefect optimice la ejecuci√≥n, maneje dependencias complejas, y construya el DAG antes de ejecutar cualquier cosa. Seg√∫n la [documentaci√≥n de Prefect](https://docs.prefect.io/latest/concepts/flows/), esto es parte del concepto de \"imperative orchestration\" donde defines el flujo de manera imperativa pero Prefect lo ejecuta de manera declarativa.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb7d4a",
   "metadata": {},
   "source": [
    "#### 3. ¬øQu√© son los Task States?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Los **Task States** representan el estado actual de una task durante su ciclo de vida. Seg√∫n la [documentaci√≥n de Prefect sobre States](https://docs.prefect.io/latest/concepts/states/), algunos estados posibles son:\n",
    "\n",
    "| Estado | ¬øCu√°ndo ocurre? |\n",
    "|--------|----------------|\n",
    "| **PENDING** | La task est√° esperando ser ejecutada. Estado inicial cuando se crea la task y est√° en cola. |\n",
    "| **RUNNING** | La task est√° actualmente en ejecuci√≥n. Ocurre cuando el c√≥digo de la task est√° siendo procesado activamente. |\n",
    "| **COMPLETED** | La task se ejecut√≥ exitosamente y retorn√≥ un resultado. Ocurre cuando la ejecuci√≥n termina sin errores. |\n",
    "| **FAILED** | La task fall√≥ durante su ejecuci√≥n debido a una excepci√≥n. Ocurre cuando se lanza una excepci√≥n no capturada. |\n",
    "| **RETRYING** | La task est√° siendo reintentada despu√©s de un fallo. Ocurre cuando se configura `retries` y la task falla, antes del siguiente intento. |\n",
    "| **CANCELLED** | La task fue cancelada antes de completarse. Ocurre cuando el flow es cancelado o interrumpido externamente. |\n",
    "| **CRASHED** | La task fall√≥ de manera inesperada o no manejada. Estado de fallo cr√≠tico cuando no hay manejo de errores. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc65b208",
   "metadata": {},
   "source": [
    "#### 4. ¬øQu√© par√°metros importantes tiene el decorador `@task`?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "\n",
    "Seg√∫n la [documentaci√≥n de Prefect Tasks](https://docs.prefect.io/latest/api-ref/prefect/tasks/), algunos par√°metros importantes son:\n",
    "\n",
    "| Par√°metro | ¬øQu√© hace? | Ejemplo de uso |\n",
    "|-----------|------------|----------------|\n",
    "| **`retries`** | Define cu√°ntas veces reintentar la task si falla. Por defecto es 0. | `@task(retries=3)` - La task intentar√° hasta 3 veces antes de fallar definitivamente. |\n",
    "| **`retry_delay_seconds`** | Tiempo de espera entre reintentos en segundos. √ötil para tareas que pueden fallar temporalmente. | `@task(retries=2, retry_delay_seconds=5)` - Espera 5 segundos entre reintentos. |\n",
    "| **`timeout_seconds`** | Tiempo m√°ximo de ejecuci√≥n para la task. Si excede, se cancela autom√°ticamente. | `@task(timeout_seconds=300)` - La task debe completarse en 5 minutos o se cancela. |\n",
    "| **`cache_key_fn`** | Funci√≥n personalizada para generar la clave de cach√©. Permite controlar cu√°ndo se cachea un resultado. | `@task(cache_key_fn=lambda task, context: context['parameters']['date'])` - Cachea basado en un par√°metro espec√≠fico. |\n",
    "| **`cache_expiration`** | Duraci√≥n para la cual el resultado cacheado es v√°lido. Puede ser un timedelta. | `@task(cache_expiration=timedelta(hours=1))` - El cache expira despu√©s de 1 hora. |\n",
    "| **`tags`** | Etiquetas para organizar y filtrar tasks. √ötil para monitoreo y organizaci√≥n. | `@task(tags=[\"extract\", \"production\"])` - Etiqueta la task para filtrado y organizaci√≥n. |\n",
    "| **`log_prints`** | Si es True, captura los prints como logs de Prefect. Facilita el debugging. | `@task(log_prints=True)` - Los prints se registran autom√°ticamente en los logs de Prefect. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e97b0b",
   "metadata": {},
   "source": [
    "### 1.2 Flows en Prefect\n",
    "\n",
    "**Documentaci√≥n oficial:** [Prefect Flows](https://docs.prefect.io/latest/concepts/flows/)\n",
    "\n",
    "#### 1. ¬øCu√°l es la diferencia entre un Flow y una Task? ¬øPor qu√© necesitamos ambos?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Seg√∫n la [documentaci√≥n de Prefect](https://docs.prefect.io/latest/concepts/flows/), un **Flow** es un contenedor que orquesta y coordina la ejecuci√≥n de m√∫ltiples Tasks. Mientras que una Task representa una unidad de trabajo individual, un Flow representa el flujo de trabajo completo que conecta m√∫ltiples Tasks.\n",
    "\n",
    "**Diferencias clave:**\n",
    "- **Task**: Unidad b√°sica de trabajo, realiza una acci√≥n espec√≠fica\n",
    "- **Flow**: Orquesta m√∫ltiples tasks, define las dependencias y el orden de ejecuci√≥n\n",
    "\n",
    "**¬øPor qu√© necesitamos ambos?**\n",
    "- Las Tasks nos permiten dividir el trabajo en componentes reutilizables y rastreables\n",
    "- Los Flows nos permiten coordinar las Tasks, manejar dependencias autom√°ticamente, y proporcionar un contexto de ejecuci√≥n unificado. Sin Flows, las Tasks ser√≠an funciones aisladas sin orquestaci√≥n.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8343d64",
   "metadata": {},
   "source": [
    "#### 2. ¬øQu√© es un \"subflow\"? ¬øCu√°ndo ser√≠a √∫til usar subflows?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Un **subflow** es un Flow que se puede llamar desde dentro de otro Flow. Seg√∫n la [documentaci√≥n de Prefect sobre subflows](https://docs.prefect.io/latest/concepts/flows/#composing-flows), cuando llamas a un flow decorado con `@flow` desde dentro de otro flow, autom√°ticamente se convierte en un subflow.\n",
    "\n",
    "**Cu√°ndo ser√≠a √∫til:**\n",
    "- **Modularidad**: Dividir un pipeline grande en m√≥dulos m√°s peque√±os y manejables\n",
    "- **Reutilizaci√≥n**: Reutilizar flujos comunes en m√∫ltiples pipelines principales\n",
    "- **Organizaci√≥n**: Organizar l√≥gicamente grupos relacionados de tasks\n",
    "- **Debugging**: Facilitar el debugging al aislar secciones del pipeline\n",
    "- **Testing**: Probar secciones espec√≠ficas del pipeline de manera independiente\n",
    "\n",
    "Por ejemplo, podr√≠as tener un subflow `extract_and_validate()` que se use en m√∫ltiples pipelines ETL diferentes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4abf491",
   "metadata": {},
   "source": [
    "#### 3. ¬øC√≥mo maneja Prefect las dependencias entre tasks? Expliquen el concepto de DAG impl√≠cito.\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Prefect maneja las dependencias entre tasks de manera **autom√°tica e impl√≠cita** a trav√©s de lo que se conoce como \"DAG impl√≠cito\". Seg√∫n la [documentaci√≥n de Prefect](https://docs.prefect.io/latest/concepts/flows/#task-dependencies), Prefect detecta autom√°ticamente las dependencias bas√°ndose en c√≥mo se llaman las tasks dentro de un flow.\n",
    "\n",
    "**C√≥mo funciona:**\n",
    "1. Cuando llamas a una task dentro de un flow y pasas el resultado de otra task como par√°metro, Prefect autom√°ticamente detecta esa dependencia\n",
    "2. Prefect construye un DAG (grafo ac√≠clico dirigido) bas√°ndose en estas dependencias impl√≠citas\n",
    "3. Las tasks se ejecutan en el orden correcto seg√∫n sus dependencias, no necesariamente en el orden en que aparecen en el c√≥digo\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "@flow\n",
    "def my_flow():\n",
    "    data = extract_data()  # Task 1\n",
    "    transformed = transform_data(data)  # Task 2 depende de Task 1\n",
    "    load_data(transformed)  # Task 3 depende de Task 2\n",
    "```\n",
    "\n",
    "Prefect detecta autom√°ticamente que `transform_data` depende de `extract_data`, y `load_data` depende de `transform_data`, construyendo un DAG sin necesidad de especificarlo expl√≠citamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0081ff5",
   "metadata": {},
   "source": [
    "### 1.3 Investigaci√≥n avanzada: Results y Caching\n",
    "\n",
    "**Documentaci√≥n oficial:** [Prefect Results](https://docs.prefect.io/latest/concepts/results/) y [Caching](https://docs.prefect.io/latest/concepts/tasks/#caching)\n",
    "\n",
    "#### 1. ¬øQu√© es el \"result persistence\"? ¬øPor qu√© es importante en pipelines de datos?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "El **result persistence** se refiere a la capacidad de Prefect de almacenar y recuperar los resultados de las tasks. Seg√∫n la [documentaci√≥n de Prefect sobre Results](https://docs.prefect.io/latest/concepts/results/), Prefect puede persistir los resultados de las tasks en diferentes backends (local, S3, GCS, etc.).\n",
    "\n",
    "**¬øPor qu√© es importante en pipelines de datos?**\n",
    "- **Resiliencia**: Si un flow falla a mitad de camino, no necesitas re-ejecutar todas las tasks desde el inicio\n",
    "- **Debugging**: Puedes inspeccionar los resultados intermedios para entender qu√© sali√≥ mal\n",
    "- **Reproducibilidad**: Puedes reproducir exactamente el mismo estado de ejecuci√≥n\n",
    "- **Eficiencia**: En caso de fallo, solo necesitas re-ejecutar desde el punto de fallo\n",
    "- **Auditor√≠a**: Permite rastrear y validar los resultados de cada etapa del pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08e11c4",
   "metadata": {},
   "source": [
    "#### 2. ¬øC√≥mo funciona el caching en Prefect? ¬øQu√© par√°metro usar√≠an para cachear el resultado de una task?\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "El **caching** en Prefect permite que las tasks eviten la re-ejecuci√≥n si ya se ejecutaron previamente con los mismos par√°metros. Seg√∫n la [documentaci√≥n de Prefect sobre Caching](https://docs.prefect.io/latest/concepts/tasks/#caching), Prefect genera autom√°ticamente una clave de cach√© bas√°ndose en los par√°metros de entrada de la task.\n",
    "\n",
    "**Par√°metros para cachear:**\n",
    "- **`cache_key_fn`**: Funci√≥n personalizada para generar la clave de cach√©\n",
    "- **`cache_expiration`**: Duraci√≥n de validez del cach√© (ej: `timedelta(hours=1)`)\n",
    "- **`refresh_cache`**: Forzar la invalidaci√≥n del cach√©\n",
    "\n",
    "**Ejemplo:**\n",
    "```python\n",
    "@task(cache_expiration=timedelta(hours=1))\n",
    "def extract_data(date: str):\n",
    "    # Esta task solo se ejecutar√° una vez por hora para la misma fecha\n",
    "    return fetch_data(date)\n",
    "```\n",
    "\n",
    "Si la misma task se llama con los mismos par√°metros dentro de la ventana de expiraci√≥n, Prefect retornar√° el resultado cacheado en lugar de re-ejecutar la task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e480026",
   "metadata": {},
   "source": [
    "#### 3. ¬øQu√© es una `cache_key_fn`? Den un ejemplo de cu√°ndo la usar√≠an.\n",
    "\n",
    "**Respuesta:**\n",
    "\n",
    "Una **`cache_key_fn`** es una funci√≥n personalizada que se usa para generar la clave de cach√© de una task. Seg√∫n la [documentaci√≥n de Prefect](https://docs.prefect.io/latest/concepts/tasks/#cache-key-functions), permite controlar exactamente qu√© factores determinan si una task debe ser re-ejecutada o usar el cach√©.\n",
    "\n",
    "**Ejemplo de uso:**\n",
    "\n",
    "```python\n",
    "@task(\n",
    "    cache_key_fn=lambda task, context: f\"{context['parameters']['date']}_{context['parameters']['region']}\"\n",
    ")\n",
    "def extract_regional_data(date: str, region: str, api_key: str):\n",
    "    # Esta task se cachea bas√°ndose en date y region, pero NO en api_key\n",
    "    return fetch_data(date, region, api_key)\n",
    "```\n",
    "\n",
    "**Cu√°ndo usarla:**\n",
    "- Cuando solo quieres cachear bas√°ndote en **algunos par√°metros** (no todos)\n",
    "- Cuando necesitas incluir factores externos (como la hora del d√≠a, estado de archivos) en la clave de cach√©\n",
    "- Cuando quieres invalidar el cach√© bas√°ndote en l√≥gica de negocio compleja\n",
    "- Por ejemplo: cachear datos de ventas solo bas√°ndote en la fecha, ignorando otros par√°metros como IDs de usuario que no afectan el resultado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59cbb2",
   "metadata": {},
   "source": [
    "## Parte 2 ‚Äî Dise√±o Conceptual (5 min)\n",
    "\n",
    "Definimos un **escenario simple** para nuestro pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b198b7",
   "metadata": {},
   "source": [
    "### 2.1 Arquitectura del escenario\n",
    "\n",
    "**Escenario elegido**: Ventas de un e-commerce con datos de transacciones diarias\n",
    "\n",
    "| Rol | ¬øQui√©n ser√≠a en nuestro escenario? |\n",
    "|-----|-----------------------------------|\n",
    "| **Business data owner** | Equipo de ventas/negocios que genera las transacciones diarias en la plataforma e-commerce |\n",
    "| **Data engineers** | Equipo de ingenier√≠a de datos que construye y mantiene el pipeline ETL para procesar y cargar las ventas |\n",
    "| **Data consumers** | Analistas de negocio, cient√≠ficos de datos y dashboards que consumen los datos procesados para an√°lisis y reportes |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3275e609",
   "metadata": {},
   "source": [
    "### 2.2 Tipo de pipeline\n",
    "\n",
    "* **Tipo elegido**: Batch\n",
    "* **Justificaci√≥n**: Las ventas se procesan diariamente en lotes. No necesitamos procesamiento en tiempo real para este caso de uso inicial. El procesamiento batch es m√°s eficiente para an√°lisis agregados y permite procesar grandes vol√∫menes de datos de manera controlada. Adem√°s, facilita la validaci√≥n y el manejo de errores antes de cargar los datos procesados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b590aaa",
   "metadata": {},
   "source": [
    "## Parte 3 ‚Äî Implementaci√≥n del Pipeline Base (20 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45948abd",
   "metadata": {},
   "source": [
    "### 3.1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d337495c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'prefect'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Instalaci√≥n de Prefect\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# !pip install -q prefect pandas\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Importar librer√≠as\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mprefect\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flow, task\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'prefect'"
     ]
    }
   ],
   "source": [
    "# Instalaci√≥n de Prefect\n",
    "# !pip install -q prefect pandas\n",
    "\n",
    "# Importar librer√≠as\n",
    "from prefect import flow, task\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Entorno configurado correctamente\")\n",
    "print(f\"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7203fdf7",
   "metadata": {},
   "source": [
    "### 3.2 Implementar Tasks\n",
    "\n",
    "Bas√°ndose en lo que investigamos en la Parte 1, implementamos las tasks con los decoradores y par√°metros correctos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TASK 1: EXTRACT ===\n",
    "# Decorador @task con par√°metros √∫tiles: tags para organizaci√≥n y log_prints para capturar prints\n",
    "@task(tags=[\"extract\", \"data-source\"], log_prints=True)\n",
    "def extract_data():\n",
    "    \"\"\"\n",
    "    Extrae datos de la fuente.\n",
    "    Simula la extracci√≥n de datos de ventas de un e-commerce.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_rows = 100\n",
    "\n",
    "    data = {\n",
    "        'fecha': pd.date_range(start='2024-01-01', periods=n_rows, freq='D'),\n",
    "        'producto': np.random.choice(['A', 'B', 'C', 'D'], n_rows),\n",
    "        'cantidad': np.random.randint(1, 50, n_rows),\n",
    "        'precio_unitario': np.random.uniform(10, 100, n_rows).round(2),\n",
    "        'region': np.random.choice(['Norte', 'Sur', 'Este', 'Oeste'], n_rows)\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"üì• Extra√≠dos {len(df)} registros\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf08548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TASK 2: TRANSFORM ===\n",
    "# Decorador @task con tags para organizaci√≥n\n",
    "@task(tags=[\"transform\", \"data-processing\"], log_prints=True)\n",
    "def transform_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aplica transformaciones a los datos.\n",
    "    Calcula totales, categor√≠as de ticket size, y limpia los datos.\n",
    "    \"\"\"\n",
    "    # Calcular total\n",
    "    df['total'] = df['cantidad'] * df['precio_unitario']\n",
    "    \n",
    "    # Categorizar por ticket size\n",
    "    df['ticket_size'] = pd.cut(\n",
    "        df['total'],\n",
    "        bins=[0, 100, 500, 1000, float('inf')],\n",
    "        labels=['Bajo', 'Medio', 'Alto', 'Muy Alto']\n",
    "    )\n",
    "    \n",
    "    # Agregar mes y d√≠a de la semana para an√°lisis\n",
    "    df['mes'] = df['fecha'].dt.month\n",
    "    df['dia_semana'] = df['fecha'].dt.day_name()\n",
    "    \n",
    "    print(f\"üîÑ Transformados {len(df)} registros\")\n",
    "    print(f\"   Total vendido: ${df['total'].sum():,.2f}\")\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff65074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TASK 3: LOAD ===\n",
    "# Decorador @task con tags y retries por si falla la escritura\n",
    "@task(tags=[\"load\", \"data-output\"], log_prints=True, retries=2, retry_delay_seconds=3)\n",
    "def load_data(df: pd.DataFrame, output_path: str = \"ventas_procesadas.csv\") -> str:\n",
    "    \"\"\"\n",
    "    Carga los datos transformados en el destino.\n",
    "    En producci√≥n, esto podr√≠a ser una base de datos, data warehouse, etc.\n",
    "    \"\"\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"üíæ Datos cargados en: {output_path}\")\n",
    "    print(f\"   Registros guardados: {len(df)}\")\n",
    "    return output_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf8599",
   "metadata": {},
   "source": [
    "### 3.3 Implementar Flow\n",
    "\n",
    "Ahora creamos el flow principal que orquesta las tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9199225c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FLOW PRINCIPAL ===\n",
    "@flow(name=\"ETL Pipeline Ventas\", log_prints=True)\n",
    "def etl_flow():\n",
    "    \"\"\"\n",
    "    Flow principal que orquesta el pipeline ETL completo.\n",
    "    Prefect detecta autom√°ticamente las dependencias entre tasks:\n",
    "    - transform_data depende de extract_data\n",
    "    - load_data depende de transform_data\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Iniciando pipeline ETL...\")\n",
    "    \n",
    "    # Paso 1: Extraer datos\n",
    "    df_raw = extract_data()\n",
    "    \n",
    "    # Paso 2: Transformar datos (depende de extract_data)\n",
    "    df_transformed = transform_data(df_raw)\n",
    "    \n",
    "    # Paso 3: Cargar datos (depende de transform_data)\n",
    "    output_file = load_data(df_transformed)\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline completado. Archivo generado: {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4758dc13",
   "metadata": {},
   "source": [
    "### 3.4 Ejecutar el Flow\n",
    "\n",
    "Ejecutamos el flow para ver c√≥mo funciona Prefect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el flow\n",
    "if __name__ == \"__main__\":\n",
    "    result = etl_flow()\n",
    "    print(f\"\\nüéâ Flow ejecutado exitosamente. Resultado: {result}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd0d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el flow (en notebook)\n",
    "result = etl_flow()\n",
    "print(f\"\\nüéâ Flow ejecutado exitosamente. Resultado: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad4324",
   "metadata": {},
   "source": [
    "### 3.4 Preguntas de observaci√≥n\n",
    "\n",
    "#### 1. ¬øQu√© observan en los logs de Prefect?\n",
    "**Respuesta:** Los logs muestran el estado de cada task (PENDING ‚Üí RUNNING ‚Üí COMPLETED), los mensajes de print capturados por `log_prints=True`, y un resumen del flujo de ejecuci√≥n. Prefect proporciona informaci√≥n detallada sobre el tiempo de ejecuci√≥n de cada task y el flujo completo.\n",
    "\n",
    "#### 2. ¬øC√≥mo se construy√≥ el DAG?\n",
    "**Respuesta:** Prefect construy√≥ el DAG autom√°ticamente bas√°ndose en las dependencias impl√≠citas detectadas cuando pasamos el resultado de una task como par√°metro de otra. El DAG es: `extract_data` ‚Üí `transform_data` ‚Üí `load_data`.\n",
    "\n",
    "#### 3. ¬øQu√© pasar√≠a si una task falla?\n",
    "**Respuesta:** Si una task falla, Prefect marca el estado como FAILED y detiene la ejecuci√≥n del flow (a menos que configuremos manejo de errores). Las tasks dependientes no se ejecutar√°n. Con `retries` configurado, Prefect reintentar√° la task antes de marcar como fallida.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04cc27ce",
   "metadata": {},
   "source": [
    "## Parte 4 ‚Äî Investigaci√≥n: Funcionalidades Avanzadas (15 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4670a038",
   "metadata": {},
   "source": [
    "### 4.1 Retries y manejo de errores\n",
    "\n",
    "**Investigaci√≥n:** [Prefect Retries](https://docs.prefect.io/latest/concepts/tasks/#retries)\n",
    "\n",
    "**Respuestas:**\n",
    "\n",
    "1. **¬øC√≥mo funcionan los retries?** Prefect permite configurar reintentos autom√°ticos cuando una task falla. Se configuran con `@task(retries=N, retry_delay_seconds=X)` donde N es el n√∫mero de reintentos y X es el tiempo de espera entre intentos.\n",
    "\n",
    "2. **¬øCu√°ndo usar retries?** √ötil para operaciones que pueden fallar temporalmente (APIs, conexiones de red, operaciones de I/O). No es recomendable para errores de l√≥gica que siempre fallar√°n.\n",
    "\n",
    "3. **Ya lo implementamos:** En la task `load_data()` configuramos `retries=2, retry_delay_seconds=3` para manejar posibles fallos en la escritura del archivo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599cf2db",
   "metadata": {},
   "source": [
    "### 4.2 Caching de resultados\n",
    "\n",
    "**Investigaci√≥n:** [Prefect Caching](https://docs.prefect.io/latest/concepts/tasks/#caching)\n",
    "\n",
    "**Respuestas:**\n",
    "\n",
    "1. **¬øC√≥mo funciona el caching?** Prefect genera autom√°ticamente una clave de cach√© basada en los par√°metros de entrada. Si la misma task se ejecuta con los mismos par√°metros, retorna el resultado cacheado en lugar de re-ejecutar.\n",
    "\n",
    "2. **Par√°metros clave:** `cache_expiration` (timedelta) y `cache_key_fn` (funci√≥n personalizada).\n",
    "\n",
    "3. **Cu√°ndo usar:** Para tasks costosas que pueden reutilizar resultados (extracciones de APIs, procesamiento pesado) cuando los par√°metros no cambian.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176f6942",
   "metadata": {},
   "source": [
    "### 4.3 Logging personalizado\n",
    "\n",
    "**Investigaci√≥n:** [Prefect Logging](https://docs.prefect.io/latest/concepts/logs/)\n",
    "\n",
    "**Respuestas:**\n",
    "\n",
    "1. **¬øC√≥mo usar logging?** Se puede usar `get_run_logger()` dentro de una task para obtener un logger estructurado, o configurar `log_prints=True` para capturar autom√°ticamente los prints.\n",
    "\n",
    "2. **Ya lo implementamos:** Usamos `log_prints=True` en todas nuestras tasks para que los prints se registren como logs estructurados de Prefect.\n",
    "\n",
    "3. **Ventajas:** Logs centralizados, niveles de log (INFO, WARNING, ERROR), y integraci√≥n con la UI de Prefect para debugging.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb51da1",
   "metadata": {},
   "source": [
    "### 4.4 Concurrencia y paralelismo\n",
    "\n",
    "**Investigaci√≥n:** [Prefect Task Runners](https://docs.prefect.io/latest/concepts/task-runners/)\n",
    "\n",
    "**Respuestas:**\n",
    "\n",
    "1. **¬øC√≥mo funciona?** Prefect soporta ejecuci√≥n concurrente usando `ConcurrentTaskRunner()` como par√°metro del flow. Las tasks independientes pueden ejecutarse en paralelo.\n",
    "\n",
    "2. **Cu√°ndo usar:** Cuando tienes m√∫ltiples tasks que no dependen una de otra y quieres reducir el tiempo total de ejecuci√≥n.\n",
    "\n",
    "3. **Ejemplo:** Si procesamos datos por regi√≥n, podemos usar `futures = [process_region.submit(r) for r in regions]` para ejecutar en paralelo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e65b93",
   "metadata": {},
   "source": [
    "## Parte 5 ‚Äî Investigaci√≥n: Deployments y Scheduling (10 min)\n",
    "\n",
    "**Documentaci√≥n:** [Deployments](https://docs.prefect.io/latest/concepts/deployments/) y [Schedules](https://docs.prefect.io/latest/concepts/schedules/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1217490e",
   "metadata": {},
   "source": [
    "### 5.1 Conceptos de Deployment\n",
    "\n",
    "**Respuestas basadas en la documentaci√≥n:**\n",
    "\n",
    "1. **¬øQu√© es un Deployment en Prefect?** Un Deployment es una configuraci√≥n que permite ejecutar un Flow de manera programada o bajo demanda. Seg√∫n la [documentaci√≥n](https://docs.prefect.io/latest/concepts/deployments/), un Deployment conecta un Flow con una configuraci√≥n de ejecuci√≥n espec√≠fica (scheduling, work pool, par√°metros). La diferencia con un Flow es que un Flow es el c√≥digo, mientras que un Deployment es la \"instalaci√≥n\" del Flow en Prefect Cloud/Server para ejecutarlo.\n",
    "\n",
    "2. **¬øQu√© es un Work Pool?** Un Work Pool es un grupo de workers que pueden ejecutar flows. Seg√∫n la [documentaci√≥n](https://docs.prefect.io/latest/concepts/work-pools/), permite organizar d√≥nde y c√≥mo se ejecutan los flows (local, servidor, cloud). Es una abstracci√≥n para manejar la infraestructura de ejecuci√≥n.\n",
    "\n",
    "3. **¬øQu√© es un Worker?** Un Worker es un proceso que ejecuta flows desde un Work Pool. Los Workers se conectan a un Work Pool y \"toman\" trabajos (flows) para ejecutarlos. La relaci√≥n es: Deployment ‚Üí Work Pool ‚Üí Worker ‚Üí Ejecuci√≥n del Flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff5336d",
   "metadata": {},
   "source": [
    "### 5.2 Scheduling\n",
    "\n",
    "**Tipos de schedules soportados por Prefect:**\n",
    "\n",
    "| Tipo de Schedule | Descripci√≥n | Ejemplo |\n",
    "|-----------------|-------------|---------|\n",
    "| **CronSchedule** | Usa sintaxis cron para definir horarios recurrentes | `cron=\"0 6 * * *\"` - Todos los d√≠as a las 6 AM |\n",
    "| **IntervalSchedule** | Ejecuta a intervalos fijos de tiempo | `interval=timedelta(hours=1)` - Cada hora |\n",
    "| **RRuleSchedule** | Usa reglas RFC 5545 (iCalendar) para horarios complejos | √ötil para horarios de negocio, excluir fines de semana |\n",
    "| **Manual** | Sin schedule, solo ejecuci√≥n manual | Para ejecuci√≥n bajo demanda |\n",
    "\n",
    "**1. ¬øC√≥mo expresar√≠an \"ejecutar todos los d√≠as a las 6 AM\" en cron?**\n",
    "**Respuesta:** `\"0 6 * * *\"` - El formato cron es: minuto hora d√≠a_mes mes d√≠a_semana\n",
    "\n",
    "**2. ¬øQu√© es `RRuleSchedule`?** \n",
    "**Respuesta:** RRuleSchedule permite definir horarios complejos usando reglas RFC 5545, como \"cada lunes y mi√©rcoles a las 9 AM, excepto feriados\" o \"√∫ltimo d√≠a del mes\". Es m√°s flexible que cron para casos de negocio complejos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499758f5",
   "metadata": {},
   "source": [
    "### 5.3 Crear un Deployment (conceptual)\n",
    "\n",
    "Bas√°ndose en la documentaci√≥n, c√≥digo para crear un deployment del flow:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7115ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: C√≥digo conceptual para crear un deployment\n",
    "# https://docs.prefect.io/latest/concepts/deployments/\n",
    "\n",
    "from prefect import flow\n",
    "from prefect.schedules import CronSchedule\n",
    "\n",
    "# Opci√≥n 1: Usando serve() - m√°s simple para desarrollo\n",
    "if __name__ == \"__main__\":\n",
    "    etl_flow.serve(\n",
    "        name=\"etl-ventas-daily\",  # nombre del deployment\n",
    "        cron=\"0 6 * * *\",  # schedule: todos los d√≠as a las 6 AM\n",
    "        tags=[\"production\", \"etl\", \"ventas\"],  # tags para organizaci√≥n\n",
    "    )\n",
    "\n",
    "# Opci√≥n 2: Usando deploy() - m√°s control para producci√≥n\n",
    "# etl_flow.deploy(\n",
    "#     name=\"etl-ventas-daily\",\n",
    "#     work_pool_name=\"default\",  # o un work pool espec√≠fico\n",
    "#     cron=\"0 6 * * *\",\n",
    "#     tags=[\"production\", \"etl\"],\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a751c5c7",
   "metadata": {},
   "source": [
    "## Parte 6 ‚Äî Extensi√≥n DataOps (15 min)\n",
    "\n",
    "Elegimos implementar la **Opci√≥n A ‚Äî Validaci√≥n con logging estructurado**, ya que es fundamental para la calidad de datos en pipelines ETL.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4172317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPCI√ìN A: VALIDACI√ìN CON LOGGING ESTRUCTURADO ===\n",
    "from prefect import get_run_logger\n",
    "\n",
    "@task(retries=1, retry_delay_seconds=2, tags=[\"validation\", \"data-quality\"], log_prints=True)\n",
    "def validate_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Valida la calidad de los datos usando logging estructurado de Prefect.\n",
    "    Esta task valida que los datos cumplan con las expectativas antes de continuar.\n",
    "    \n",
    "    Basado en: https://docs.prefect.io/latest/concepts/logs/\n",
    "    \"\"\"\n",
    "    logger = get_run_logger()  # Obtiene el logger estructurado de Prefect\n",
    "    errors = []\n",
    "\n",
    "    logger.info(\"Iniciando validaci√≥n de datos\")\n",
    "    logger.info(f\"DataFrame recibido con {len(df)} registros y {len(df.columns)} columnas\")\n",
    "\n",
    "    # Validaci√≥n 1: DataFrame no vac√≠o\n",
    "    if len(df) <= 0:\n",
    "        logger.error(\"DataFrame vac√≠o detectado - No se pueden procesar datos vac√≠os\")\n",
    "        errors.append(\"DataFrame vac√≠o\")\n",
    "    else:\n",
    "        logger.info(f\"‚úÖ Validaci√≥n de cantidad de registros: OK ({len(df)} registros)\")\n",
    "\n",
    "    # Validaci√≥n 2: Valores nulos\n",
    "    null_counts = df.isnull().sum()\n",
    "    if null_counts.sum() > 0:\n",
    "        logger.warning(f\"Valores nulos encontrados: {null_counts[null_counts > 0].to_dict()}\")\n",
    "        # No agregamos a errors si hay nulos, solo lo registramos como warning\n",
    "        # En producci√≥n, podr√≠as decidir si esto es cr√≠tico o no\n",
    "    else:\n",
    "        logger.info(\"‚úÖ Validaci√≥n de valores nulos: OK (sin nulos)\")\n",
    "\n",
    "    # Validaci√≥n 3: Columnas requeridas\n",
    "    required_columns = ['fecha', 'producto', 'cantidad', 'precio_unitario', 'total']\n",
    "    missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "    if missing_columns:\n",
    "        logger.error(f\"Columnas requeridas faltantes: {missing_columns}\")\n",
    "        errors.append(f\"Columnas faltantes: {missing_columns}\")\n",
    "    else:\n",
    "        logger.info(f\"‚úÖ Validaci√≥n de columnas requeridas: OK\")\n",
    "\n",
    "    # Validaci√≥n 4: Tipos de datos b√°sicos\n",
    "    if 'total' in df.columns:\n",
    "        if df['total'].dtype not in [float, 'float64', 'float32', int, 'int64', 'int32']:\n",
    "            logger.error(f\"Tipo de dato incorrecto para 'total': {df['total'].dtype}\")\n",
    "            errors.append(\"Tipo de dato incorrecto en columna 'total'\")\n",
    "        else:\n",
    "            logger.info(\"‚úÖ Validaci√≥n de tipos de datos: OK\")\n",
    "\n",
    "    # Validaci√≥n 5: Valores negativos en campos que no deber√≠an tenerlos\n",
    "    if 'cantidad' in df.columns:\n",
    "        negative_qty = (df['cantidad'] < 0).sum()\n",
    "        if negative_qty > 0:\n",
    "            logger.warning(f\"Se encontraron {negative_qty} registros con cantidad negativa\")\n",
    "            # Podr√≠as decidir si esto es un error cr√≠tico o solo un warning\n",
    "\n",
    "    if errors:\n",
    "        logger.error(f\"Validaci√≥n fallida con {len(errors)} error(es) cr√≠tico(s)\")\n",
    "        raise ValueError(f\"Validaci√≥n fallida: {errors}\")\n",
    "\n",
    "    logger.info(\"‚úÖ Validaci√≥n exitosa - Todos los checks pasaron\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad913c9",
   "metadata": {},
   "source": [
    "Ahora actualizamos el flow para incluir la validaci√≥n:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e15a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FLOW ACTUALIZADO CON VALIDACI√ìN ===\n",
    "@flow(name=\"ETL Pipeline Ventas con Validaci√≥n\", log_prints=True)\n",
    "def etl_flow_with_validation():\n",
    "    \"\"\"\n",
    "    Flow principal que orquesta el pipeline ETL completo con validaci√≥n de calidad.\n",
    "    Incluye validaci√≥n entre extract y transform para asegurar calidad de datos.\n",
    "    \"\"\"\n",
    "    print(\"üöÄ Iniciando pipeline ETL con validaci√≥n...\")\n",
    "    \n",
    "    # Paso 1: Extraer datos\n",
    "    df_raw = extract_data()\n",
    "    \n",
    "    # Paso 2: Validar datos (EXTENSI√ìN DATAOPS)\n",
    "    # Esta validaci√≥n asegura que los datos cumplen con las expectativas antes de transformar\n",
    "    df_validated = validate_data(df_raw)\n",
    "    \n",
    "    # Paso 3: Transformar datos (solo si la validaci√≥n pasa)\n",
    "    df_transformed = transform_data(df_validated)\n",
    "    \n",
    "    # Paso 4: Cargar datos\n",
    "    output_file = load_data(df_transformed)\n",
    "    \n",
    "    print(f\"‚úÖ Pipeline completado con validaci√≥n. Archivo generado: {output_file}\")\n",
    "    return output_file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69110881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar el flow con validaci√≥n\n",
    "result = etl_flow_with_validation()\n",
    "print(f\"\\nüéâ Flow con validaci√≥n ejecutado exitosamente. Resultado: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef25b499",
   "metadata": {},
   "source": [
    "## Parte 7 ‚Äî Reflexi√≥n y Conexi√≥n con DataOps (5 min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c44906",
   "metadata": {},
   "source": [
    "### 7.1 Conceptos de Prefect\n",
    "\n",
    "**1. ¬øC√≥mo ayuda Prefect a implementar el principio de \"Observabilidad\" de DataOps?**\n",
    "\n",
    "**Respuesta:** Prefect implementa observabilidad de m√∫ltiples formas:\n",
    "- **Logging estructurado**: Los logs de Prefect capturan autom√°ticamente el estado, tiempo de ejecuci√≥n y mensajes de cada task\n",
    "- **Estados de ejecuci√≥n**: Cada task tiene estados claros (PENDING, RUNNING, COMPLETED, FAILED) que se pueden monitorear\n",
    "- **UI centralizada**: Prefect Cloud/Server proporciona una interfaz visual para monitorear todos los flows y tasks en tiempo real\n",
    "- **M√©tricas**: Prefect rastrea m√©tricas como tiempo de ejecuci√≥n, tasa de √©xito, y frecuencias de fallo\n",
    "- **Notificaciones**: Permite configurar alertas cuando los flows fallan\n",
    "\n",
    "Esto permite que los equipos de datos tengan visibilidad completa sobre sus pipelines, facilitando el debugging y la detecci√≥n temprana de problemas.\n",
    "\n",
    "**2. ¬øC√≥mo ayuda el caching a la \"Reproducibilidad\"?**\n",
    "\n",
    "**Respuesta:** El caching en Prefect ayuda a la reproducibilidad de varias formas:\n",
    "- **Resultados consistentes**: Al cachear resultados basados en par√°metros, aseguras que las mismas entradas produzcan los mismos resultados\n",
    "- **Traza de ejecuciones**: Prefect mantiene un historial de ejecuciones cacheadas, permitiendo reproducir estados anteriores\n",
    "- **Puntos de recuperaci√≥n**: Si un pipeline falla, puedes reutilizar resultados cacheados de tasks exitosas anteriores\n",
    "- **Validaci√≥n**: Permite comparar resultados actuales con resultados cacheados para detectar cambios inesperados\n",
    "\n",
    "Esto es fundamental en DataOps donde necesitas garantizar que los pipelines produzcan resultados consistentes y reproducibles.\n",
    "\n",
    "**3. ¬øC√≥mo conectan los Deployments con \"CI/CD para datos\"?**\n",
    "\n",
    "**Respuesta:** Los Deployments en Prefect son el equivalente a \"releases\" en CI/CD tradicional:\n",
    "- **Versionado**: Los deployments permiten versionar y desplegar diferentes versiones de flows\n",
    "- **Automatizaci√≥n**: Los schedules permiten automatizar la ejecuci√≥n de pipelines, similar a pipelines de CI/CD\n",
    "- **Ambientes**: Puedes tener diferentes deployments para desarrollo, staging y producci√≥n\n",
    "- **Rollback**: Puedes cambiar qu√© versi√≥n de un flow est√° activa, permitiendo rollback r√°pido\n",
    "- **Integraci√≥n con CI/CD**: Prefect puede integrarse con sistemas de CI/CD (GitHub Actions, GitLab CI) para desplegar autom√°ticamente cambios en los flows\n",
    "\n",
    "Esto permite aplicar pr√°cticas de DevOps a los pipelines de datos, facilitando despliegues seguros y controlados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023e7f51",
   "metadata": {},
   "source": [
    "### 7.2 Comparaci√≥n con alternativas\n",
    "\n",
    "**1. ¬øQu√© diferencias hay entre Prefect y Apache Airflow? Mencionen al menos 2.**\n",
    "\n",
    "**Diferencia 1:** **Filosof√≠a de dise√±o**: Airflow usa DAGs expl√≠citos definidos en Python usando operadores, mientras que Prefect usa \"DAGs impl√≠citos\" donde las dependencias se detectan autom√°ticamente bas√°ndose en c√≥mo se llaman las tasks. Prefect es m√°s \"Python puro\" y menos verboso.\n",
    "\n",
    "**Diferencia 2:** **Curva de aprendizaje**: Prefect tiene una curva de aprendizaje m√°s suave porque es m√°s simple de usar - solo decoras funciones con `@task` y `@flow`. Airflow requiere entender conceptos como DAGs, Operators, XComs, etc., lo que lo hace m√°s complejo para empezar.\n",
    "\n",
    "**Otra diferencia importante:** Prefect est√° dise√±ado para ser m√°s moderno y orientado a Python nativo, mientras que Airflow fue dise√±ado originalmente m√°s como un sistema de workflow basado en DAGs.\n",
    "\n",
    "**2. ¬øQu√© es Dagster? ¬øEn qu√© se diferencia de Prefect?**\n",
    "\n",
    "**Respuesta:** Dagster es otra herramienta de orquestaci√≥n de pipelines de datos moderna, similar a Prefect. Las principales diferencias son:\n",
    "\n",
    "- **Enfoque en assets**: Dagster est√° m√°s orientado a \"assets\" (datos y modelos) como primitivos de primera clase, mientras que Prefect est√° m√°s orientado a \"tasks\" y \"flows\"\n",
    "- **Integraci√≥n con dataframes**: Dagster tiene integraci√≥n m√°s profunda con bibliotecas de datos como pandas, pyspark, etc.\n",
    "- **Software-defined assets**: Dagster permite definir pipelines bas√°ndose en qu√© datos (assets) necesitas, no solo en qu√© tareas ejecutar\n",
    "- **Ecosistema**: Prefect tiene un ecosistema m√°s amplio y comunidad m√°s grande, mientras que Dagster est√° m√°s enfocado en equipos de datos grandes\n",
    "\n",
    "Ambas son alternativas modernas a Airflow, pero Dagster tiene un enfoque m√°s \"data-centric\" mientras que Prefect es m√°s \"workflow-centric\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b276ad08",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En esta tarea aprendimos:\n",
    "\n",
    "1. **Conceptos fundamentales** de Prefect: Tasks, Flows, DAGs impl√≠citos, estados, y caching\n",
    "2. **Implementaci√≥n pr√°ctica** de un pipeline ETL completo con Prefect\n",
    "3. **Funcionalidades avanzadas**: Retries, logging estructurado, validaci√≥n de datos\n",
    "4. **Deployments y scheduling**: C√≥mo desplegar y programar pipelines en producci√≥n\n",
    "5. **Conexi√≥n con DataOps**: C√≥mo Prefect habilita observabilidad, reproducibilidad y CI/CD para datos\n",
    "\n",
    "Prefect es una herramienta poderosa para orquestar pipelines de datos de manera moderna, con una curva de aprendizaje suave y excelente integraci√≥n con el ecosistema Python.\n",
    "\n",
    "---\n",
    "\n",
    "**Referencias:**\n",
    "- [Documentaci√≥n oficial de Prefect](https://docs.prefect.io/)\n",
    "- [Prefect Concepts Overview](https://docs.prefect.io/latest/concepts/)\n",
    "- [Prefect GitHub Examples](https://github.com/PrefectHQ/prefect)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32a0e09",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e99fb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065bebce",
   "metadata": {},
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
